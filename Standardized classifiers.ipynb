{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for Human Activity Recognition\n",
    "\n",
    "Human activity recognition using smartphones dataset and an LSTM RNN. Classifying the type of movement amongst six categories:\n",
    "- WALKING,\n",
    "- WALKING_UPSTAIRS,\n",
    "- WALKING_DOWNSTAIRS,\n",
    "- SITTING,\n",
    "- STANDING,\n",
    "- LAYING.\n",
    "\n",
    "Compared to a classical approach, using a Recurrent Neural Networks (RNN) with Long Short-Term Memory cells (LSTMs) require no or almost no feature engineering. Data can be fed directly into the neural network who acts like a black box, modeling the problem correctly. Other research on the activity recognition dataset used mostly use a big amount of feature engineering, which is rather a signal processing approach combined with classical data science techniques. The approach here is rather very simple in terms of how much did the data was preprocessed. \n",
    "\n",
    "## Video dataset overview\n",
    "\n",
    "Follow this link to see a video of the 6 activities recorded in the experiment with one of the participants:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=XOEN9W05_4A\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/XOEN9W05_4A/0.jpg\" \n",
    "alt=\"Video of the experiment\" width=\"400\" height=\"300\" border=\"10\" /></a>\n",
    "  <a href=\"https://youtu.be/XOEN9W05_4A\"><center>[Watch video]</center></a>\n",
    "</p>\n",
    "\n",
    "## Details about input data\n",
    "\n",
    "I will be using an LSTM on the data to learn (as a cellphone attached on the waist) to recognise the type of activity that the user is doing. The dataset's description goes like this:\n",
    "\n",
    "> The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. \n",
    "\n",
    "That said, I will use the almost raw data: only the gravity effect has been filtered out of the accelerometer  as a preprocessing step for another 3D feature as an input to help learning. \n",
    "\n",
    "## What is an RNN?\n",
    "\n",
    "As explained in [this article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), an RNN takes many input vectors to process them and output other vectors. It can be roughly pictured like in the image below, imagining each rectangle has a vectorial depth and other special hidden quirks in the image below. **In our case, the \"many to one\" architecture is used**: we accept time series of feature vectors (one vector per time step) to convert them to a probability vector at the output for classification. Note that a \"one to one\" architecture would be a standard feedforward neural network. \n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" />\n",
    "\n",
    "An LSTM is an improved RNN. It is more complex, but easier to train, avoiding what is called the vanishing gradient problem. \n",
    "\n",
    "\n",
    "## Results \n",
    "\n",
    "Scroll on! Nice visuals awaits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#loader class for loading and preprocessing the data sets\n",
    "from loader import Loader\n",
    "import time\n",
    "\n",
    "# All Includes\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import layers\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Reshape, Dropout, Conv1D, MaxPool1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "\n",
    "np.random.seed(1337)\n",
    "tf.set_random_seed(1339)\n",
    "\n",
    "session_conf = tf.ConfigProto(log_device_placement=True)\n",
    "# # Avoid full memory allocation on GPU\n",
    "session_conf.gpu_options.allow_growth=True\n",
    "\n",
    "sess = tf.InteractiveSession(config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a dataset and preparing it:\n",
    "\n",
    "1) Run the appropriate loader from Loader  class <br>\n",
    "2) Run the appropriate preprocessor (sliding window size , overlay) <br>\n",
    "3) Load the pickled output file of the loader <br>\n",
    "\n",
    "Could potentially be used in a loop for automated testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "load = Loader()\n",
    "\n",
    "sliding_window = 5\n",
    "overlay = 1\n",
    "\n",
    "\n",
    "#load.ward1_load()\n",
    "#load.ward1_preprocess(sliding_window, overlay) # 40,20\n",
    "#path = \"WARD1\"\n",
    "#load.opportunity_load()\n",
    "#load.opportunity_preprocess(sliding_window, overlay) # 300, 80\n",
    "#path = \"OpportunityUCIDataset\"\n",
    "#load.daily_sports_load()\n",
    "#load.daily_sports_preprocess(sliding_window, overlay)  # 25, 15\n",
    "#path = \"Daily and sports activities\"\n",
    "#load.pamap2_load()\n",
    "#load.pamap2_preprocess(sliding_window,  overlay)  # 270, 30\n",
    "#path = \"PAMAP2\"\n",
    "\n",
    "#load.chiron_first_load()\n",
    "#frame = load.chiron_first_preprocess(sliding_window, overlay)  # 10,5\n",
    "#path = \"Chiron first\"\n",
    "#load.chiron_second_load()\n",
    "#frame = load.chiron_second_preprocess(sliding_window, overlay)  # 1,0\n",
    "path = \"Chiron second\"\n",
    "\n",
    "\n",
    "#df = pickle.load(open('WARD1/processed_data.txt','rb')) # -> works\n",
    "#df = pickle.load(open('OpportunityUCIDataset/full_preprocessed_dataset.txt','rb'))\n",
    "#df = pickle.load(open('Daily and sports activities/full_preprocessed.txt','rb'))\n",
    "#df = pickle.load(open('PAMAP2/full_preprocessed.txt','rb'))\n",
    "\n",
    "#df = pickle.load(open('Chiron first/Chiron_full_preprocessed.txt' ,'rb'))\n",
    "df = pickle.load(open('Chiron second/Chiron_full_preprocessed.txt' ,'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "7.0     7806\n",
      "9.0     4612\n",
      "1.0     4464\n",
      "32.0    3637\n",
      "11.0    3152\n",
      "12.0    3140\n",
      "16.0    3116\n",
      "17.0    2637\n",
      "3.0     2515\n",
      "26.0    2233\n",
      "13.0    2180\n",
      "8.0     2091\n",
      "31.0    2047\n",
      "25.0    2030\n",
      "5.0     1818\n",
      "4.0     1723\n",
      "18.0    1679\n",
      "14.0    1597\n",
      "27.0    1457\n",
      "2.0     1310\n",
      "10.0    1276\n",
      "28.0    1264\n",
      "30.0    1167\n",
      "15.0    1072\n",
      "29.0     974\n",
      "23.0     876\n",
      "6.0      850\n",
      "24.0     740\n",
      "20.0     431\n",
      "22.0     399\n",
      "21.0     283\n",
      "19.0     186\n",
      "Name: Activity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print(df)\n",
    "print(\"------------------------------------------------------\")\n",
    "print(df.Activity.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the leave one out method:\n",
    "\n",
    "Using methods that prepare the next leave one out division <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df.values[:,0])\n",
    "\n",
    "classes = len(encoder.classes_)\n",
    "class_array = (np.array(df.Activity.unique()).astype(int) - 1)\n",
    "\n",
    "def train_test(data, index):\n",
    "    #training and test sets -> leave one out\n",
    "    leave_out = df.index.isin([index])\n",
    "\n",
    "    X_train = df[~leave_out].values[:,2:]\n",
    "    y_train = df[~leave_out].values[:,0]\n",
    "\n",
    "    X_test = df[leave_out].values[:,2:]\n",
    "    y_test = df[leave_out].values[:,0]\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "\n",
    "    #normalization\n",
    "    X_test = normalize(X_test)\n",
    "    X_train = normalize(X_train)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    #reshaping to fit keras\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29 30 31 32]\n"
     ]
    }
   ],
   "source": [
    "print(class_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additionnal Parameters:\n",
    "\n",
    "Here are some core parameter definitions for the training. \n",
    "\n",
    "The whole neural network's structure could be summarised by enumerating those parameters and the fact an LSTM is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def additional_parameters(X_train, X_test):\n",
    "\n",
    "    training_data_count = X_train.shape[0]  # 21522 training series (with 50% overlap between each serie)\n",
    "    test_data_count = X_test.shape[0]  # 7794 testing series\n",
    "\n",
    "    n_steps = X_train.shape[1]  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])  # 7 input parameters per timestep\n",
    "\n",
    "    print(\"Steps: \",n_steps, \"N imput\",n_input)\n",
    "    \n",
    "    return training_data_count, test_data_count, n_steps, n_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\n"
     ]
    }
   ],
   "source": [
    "# Input Data \n",
    "\n",
    "# LSTM Neural Network's internal structure\n",
    "\n",
    "n_hidden = 32 # Hidden layer num of features\n",
    "n_classes = classes # Total classes (should go up, or should go down)\n",
    "\n",
    "# Training \n",
    "\n",
    "learning_rate = 0.0025\n",
    "lambda_loss_amount = 0.0015\n",
    "training_iters = 300  # Loop 300 times on the dataset\n",
    "batch_size = 1500\n",
    "display_iter = 30000  # To show test set accuracy during training\n",
    "\n",
    "\n",
    "# Some debugging info\n",
    "\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "#print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test)) # -> takes too long\n",
    "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(y_):\n",
    "    #encoder = LabelEncoder()\n",
    "    y_encoded = encoder.transform(y_.ravel())#fit_transform(y_.ravel())\n",
    "    return np_utils.to_categorical(y_encoded, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic regression network\n",
    "\n",
    "1) Get training and test data sets and calculate some additional parameters <br>\n",
    "2) Use the additional parameters to build a model <br>\n",
    "3) Train, test and evaluate the model <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:  181 N imput 1\n",
      "(58858, 32)\n",
      "(5904, 32)\n",
      "Started fitting model (leave out subject: 1) ....... 1\n",
      "\n",
      "Confusion matrix:\n",
      "     0    1   2    3   4    5    6   7    8    9  ...  22  23  24  25  26  27  \\\n",
      "0   155    0   0    0   0    3    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "1     0  282   0    0   0    7    0   0    0   13 ...   0   0   0   0   0   0   \n",
      "2     0    0  53    0   0  210    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "3     0    0   0  194   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "4     0    0   0    0  18  126    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "5     0    0   0    0   1  828    0   0    0    0 ...   0   0   0   0   0   3   \n",
      "6    36    1   0    0   0    0  222   0    0    0 ...   0   0   0   0   0   0   \n",
      "7     0    0   0    0   0    0    0  85   29    0 ...   0   0   0   0   0   0   \n",
      "8     0    0   0    0   8    5    0   0  131    0 ...   0   0   0   0   0   0   \n",
      "9     0    4   1    3   0   23   24   0    0  330 ...   0   0   0   0   0   0   \n",
      "10    7    1   2    5  13  140    9   0    0   94 ...   0   0   0   0   0   5   \n",
      "11    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "12    0    0   0    0   0   17    0  62   49    0 ...   0   0   0   0   0   0   \n",
      "13    0    0   0    0  13    4    0   2   32    0 ...   0   0   0   0   0   0   \n",
      "14    0    0   0    0   0    0    0  23    0    0 ...   0   0   0   0   0   0   \n",
      "15    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "16    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "17    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "18    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "19    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "20    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "21    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "22    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "23    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "24    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "25    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "26    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "27    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "28    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "29    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "30    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "31    0    0   0    0   0    0    0   0    0    0 ...   0   0   0   0   0   0   \n",
      "\n",
      "    28  29  30  31  \n",
      "0    0   0   0   0  \n",
      "1    6   0   0   0  \n",
      "2    0   0   0   0  \n",
      "3    0   0   0   0  \n",
      "4    0   0   0   0  \n",
      "5    0  14   0   0  \n",
      "6    0   0   0   0  \n",
      "7    0   0   0   0  \n",
      "8    0   0   0   0  \n",
      "9    0   0   0   0  \n",
      "10   2   0   0   0  \n",
      "11   0   0   0   0  \n",
      "12   0   0   0   0  \n",
      "13   1  11   0   0  \n",
      "14   0   0   0   0  \n",
      "15   0   0   0   0  \n",
      "16   0   0   0   0  \n",
      "17   0   0   0   0  \n",
      "18   0   0   0   0  \n",
      "19   0   0   0   0  \n",
      "20   0   0   0   0  \n",
      "21   0   0   0   0  \n",
      "22   0   0   0   0  \n",
      "23   0   0   0   0  \n",
      "24   0   0   0   0  \n",
      "25   0   0   0   0  \n",
      "26   0   0   0   0  \n",
      "27   0   0   0   0  \n",
      "28   0   0   0   0  \n",
      "29   0   0   0   0  \n",
      "30   0   0   0   0  \n",
      "31   0   0   0   0  \n",
      "\n",
      "[32 rows x 32 columns]\n",
      "F1 score ....... 0.65108401084\n",
      "\n",
      "Finished fitting model (leave out subject: 1) ....... 1\n",
      "Total time: 47.732030391693115\n",
      "------------------------------------------------------------\n",
      "Steps:  181 N imput 1\n",
      "(48785, 32)\n",
      "(15977, 32)\n",
      "Started fitting model (leave out subject: 2) ....... 2\n",
      "\n",
      "Confusion matrix:\n",
      "    0    1   2    3   4     5    6    7    8    9  ...   22   23   24   25  \\\n",
      "0   72    0   0    0   0    98   32    7   24   26 ...    0    0    0    0   \n",
      "1    0  428   0    4   0     0    0    0    0    0 ...    0    0    0    0   \n",
      "2    3    0  33    0  37   188    0    0    0    0 ...    0    0    0    0   \n",
      "3    0   28   0  274   0     0    0    0    0   18 ...    0    0    0    0   \n",
      "4    0    0   0    0   3   115    0    0   15    0 ...    0    0    0    0   \n",
      "5    0    0   0    0   1  1917    0    0   16    1 ...    0    0    0    0   \n",
      "6    0    0   0    0   0     0  331    0    0   62 ...    0    0    0    0   \n",
      "7    0    0   0    0   2     0    0  957   37    0 ...    0    0    0    0   \n",
      "8    0    0   0    0   0     0    0    0  168    0 ...    0    0    0    0   \n",
      "9   11    0   0    0   0     0    0    0    0  365 ...    0    0    0    0   \n",
      "10  14    1   2    0   5    33   18    0   10   89 ...    0    0    2    0   \n",
      "11   0    0   0    0   0     0    0    0    0    0 ...    0    0    0    0   \n",
      "12   0    0   0    0   0    17    0   30    1    0 ...    0    0    0    0   \n",
      "13   0    0   0    0   0    22    0    2   31    0 ...    0    0    0    0   \n",
      "14   0    0   0    0   0    14    0    0    7    0 ...    0    0    1    0   \n",
      "15   0    0   0    0   0     0    0    0    0    0 ...    0    0    0    0   \n",
      "16   0    0   0    0   4    76    0   24   21    8 ...    0    0    0    0   \n",
      "17   0    0   0    0   0     0    0    0    0    0 ...    0    0    0    0   \n",
      "18   0    0   0    0   0     0    0    0   10    0 ...    5    0    0    0   \n",
      "19   0    0   0    0   0     0    0    9    0    0 ...    0    0    0    0   \n",
      "20   0    0   0    0   0     0    0    0    0    0 ...    0    0    0    0   \n",
      "21   0    2   0    0   0     2   12    0    0   18 ...    0    0    0    0   \n",
      "22   0    0   0    0   0     0    0    0    0    0 ...  281    0    0    0   \n",
      "23   0    0   0    0   0     0    0    0    0    0 ...    0  443    0  204   \n",
      "24   0    0   0    0   0     0    0    0    0    0 ...    0    0  653    0   \n",
      "25   0    0   0    0   0     0    0    0    0    0 ...    0  209    0  283   \n",
      "26   0    0   0    0   0     0    0    0    0    0 ...    0    0  145    0   \n",
      "27   0    0   0    0   0     9    0    0    0    0 ...    0    0    0    0   \n",
      "28   0    0   0    0   0   162    0    0    0    0 ...    0    0    0    0   \n",
      "29   0    0   0    0   0   294    0    0    0    0 ...    0    0    0    0   \n",
      "30  20    0   0    0   0     0    0    0    0    0 ...    0    0    0    0   \n",
      "31   0    0   0    0   0     0    0    0    0    0 ...    0    0    0    0   \n",
      "\n",
      "     26   27  28   29    30  31  \n",
      "0     0    0   0    0    41   0  \n",
      "1     0   10   0    0     0   0  \n",
      "2     0    0   6   14     0   0  \n",
      "3     0    0   0    0     0   0  \n",
      "4     0    2   0    3     2   0  \n",
      "5     0   14   2    7     0   0  \n",
      "6     0    0   0    0     0   0  \n",
      "7     0    0   0    0     0   0  \n",
      "8     0    0   0    0     0   0  \n",
      "9     0    0   5   10     0   0  \n",
      "10    0    0  15   20     0   0  \n",
      "11    0    0   0    0     0   0  \n",
      "12    0    0   0    0     0   0  \n",
      "13    0    0   0   16     4   0  \n",
      "14    0    1   0    0     0   0  \n",
      "15    0    0   0    0     0   0  \n",
      "16    0    0   0    0     0   0  \n",
      "17    0    0   0    0     0   0  \n",
      "18    0    0   0    0     0   0  \n",
      "19    0    0   0    0     0   0  \n",
      "20    0    0   0    0     0   0  \n",
      "21    0    0   0    0     0   0  \n",
      "22    0    0   0    0     0   0  \n",
      "23    0    0   0    0     0   0  \n",
      "24  222    0   0    0     0   0  \n",
      "25    0    0   0    0     0   0  \n",
      "26  197    0   0    0     0   0  \n",
      "27    0  143   0    0   137   0  \n",
      "28    0   11   5   17     0   0  \n",
      "29    0    3   0  415    16   0  \n",
      "30    0    0   0    0  1446   0  \n",
      "31    0    0   0    0     0   0  \n",
      "\n",
      "[32 rows x 32 columns]\n",
      "F1 score ....... 0.721286849847\n",
      "\n",
      "Finished fitting model (leave out subject: 2) ....... 2\n",
      "Total time: 43.3771185874939\n",
      "------------------------------------------------------------\n",
      "Steps:  181 N imput 1\n",
      "(56343, 32)\n",
      "(8419, 32)\n",
      "Started fitting model (leave out subject: 3) ....... 3\n",
      "\n",
      "Confusion matrix:\n",
      "    0    1    2    3   4    5    6    7   8    9  ...  22  23  24  25  26  27  \\\n",
      "0   11    0    0    0   0    0   11    0   0  113 ...   0   0   0   0   0   0   \n",
      "1    0  529    0    0   0    0    0    0   0   51 ...   0   0   0   0   0   6   \n",
      "2    0    0   54    0   9  136    0    0   0    7 ...   0   0   0   0   0  23   \n",
      "3    0    0    0  328   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "4    0    0    6    5  14   68    1    0   0    2 ...   0   0   0   0   0   1   \n",
      "5    5    0  112    0   2  459    0    0   0    4 ...   0   0   5   0   0  28   \n",
      "6    0    0    0    0   0    0  328    0   0   27 ...   0   0   0   0   0   0   \n",
      "7    0    0    0    0   0    0    0   98   1    0 ...   0   0   0   0   0   0   \n",
      "8    0    0    2    0   8    2    0  218  19    0 ...   0   0   0   0   0   8   \n",
      "9    0   19    0    0   0   11   18    0   0  413 ...   0   0   0   0   0   0   \n",
      "10   0    7    0    7   2    4   82    0   0  140 ...   0   0   0   0   0   0   \n",
      "11   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "12   0    0    0    0   0   19    0   39  11    0 ...   0   0  22   0   0   0   \n",
      "13   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0  29   \n",
      "14   0    0    0    0   0    0    0   64   0    0 ...   3   0   0   0   0   0   \n",
      "15   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "16   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "17   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "18   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "19   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "20   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "21   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "22   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "23   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "24   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "25   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "26   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "27   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "28   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "29   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "30   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "31   0    0    0    0   0    0    0    0   0    0 ...   0   0   0   0   0   0   \n",
      "\n",
      "    28  29  30  31  \n",
      "0    0   0  60   0  \n",
      "1    9   0   0   0  \n",
      "2    0   0   0   0  \n",
      "3    0   0   0   0  \n",
      "4    0   0   0   0  \n",
      "5    0  60  43   0  \n",
      "6    0   0   0   0  \n",
      "7    0   0   0   0  \n",
      "8    0   0   0   0  \n",
      "9    0   0   0   0  \n",
      "10   1   2   3   0  \n",
      "11   0   0   0   0  \n",
      "12   1   2  29   0  \n",
      "13   0   3   0   0  \n",
      "14   0   0   0   0  \n",
      "15   0   0   0   0  \n",
      "16   0   0   0   0  \n",
      "17   0   0   0   0  \n",
      "18   0   0   0   0  \n",
      "19   0   0   0   0  \n",
      "20   0   0   0   0  \n",
      "21   0   0   0   0  \n",
      "22   0   0   0   0  \n",
      "23   0   0   0   0  \n",
      "24   0   0   0   0  \n",
      "25   0   0   0   0  \n",
      "26   0   0   0   0  \n",
      "27   0   0   0   0  \n",
      "28   0   0   0   0  \n",
      "29   0   0   0   0  \n",
      "30   0   0   0   0  \n",
      "31   0   0   0   0  \n",
      "\n",
      "[32 rows x 32 columns]\n",
      "F1 score ....... 0.493051431286\n",
      "\n",
      "Finished fitting model (leave out subject: 3) ....... 3\n",
      "Total time: 50.092205286026\n",
      "------------------------------------------------------------\n",
      "Steps:  181 N imput 1\n",
      "(49331, 32)\n",
      "(15431, 32)\n",
      "Started fitting model (leave out subject: 4) ....... 4\n",
      "\n",
      "Confusion matrix:\n",
      "     0    1   2    3   4     5    6    7    8    9  ...   22   23   24   25  \\\n",
      "0   145    0   0    0   0    38    0    0    0    0 ...    0    4    0    0   \n",
      "1     0  523   0    0   0     0    0    0    0    0 ...    0    0    0    0   \n",
      "2     0    0  40    0   0   259    0    0    0    0 ...    0    0    0    0   \n",
      "3     0  173   0  115   0    16    0    0    0    0 ...    0    0    0    0   \n",
      "4     0    0   2    0  49    68    0    0    0    0 ...    0    0    0    0   \n",
      "5     3    0   5    0  27  1542    0    2    0    3 ...    0    0    0    0   \n",
      "6     0   50   0    0   0     0  247    0    0    0 ...    0    0    0    0   \n",
      "7     0    0   0    0   0     0    0  900   49    0 ...    0    0    0    0   \n",
      "8     0    0   0    0   7     0    0    0  111    0 ...    0    0    0    0   \n",
      "9     0    0   1    0   0     5   11    0    0  211 ...    0    0    0    0   \n",
      "10    1    4   5    2   1   146    3    0    0  131 ...    0    0    1    0   \n",
      "11    0    0   0    0   0     0    0    0    0    0 ...    0    0    0    0   \n",
      "12    0    0   0    0   0     7    0   30    0    0 ...    0    0    0    0   \n",
      "13    0    0   0    0   7     0    0    0    0    0 ...    0    0    0    0   \n",
      "14    0    0   0    0   0     1    0   18    0    0 ...    0    0    0    0   \n",
      "15    0    0   0    0   0     0    0    0    0    0 ...    6    0    0    0   \n",
      "16    0    0   0    0   5    45    2   56    8    1 ...    1    8    0    8   \n",
      "17    0    0   0    0   0     0    0    0    0    0 ...    0    0    0    0   \n",
      "18    0    0   0    0   0     0    0   17    0    0 ...    0    0    0    0   \n",
      "19    0    0   0    0   0     0    0   31    0    0 ...    0    0    0    0   \n",
      "20    0    0   0    0   0     0    0    0    0    0 ...    0    0    0    0   \n",
      "21    0    0   0    2   0    19   15    0    0   20 ...    0    0    0    0   \n",
      "22    0    0   0    0   0     0    0    4    0    0 ...  235    0    0    0   \n",
      "23    0    0   0    0   0     1    0    0    0    0 ...    0  708    0   21   \n",
      "24    0    0   0    0   0     0    0    0    0    0 ...    0    0  848    0   \n",
      "25    0    0   0    0   0     0    0    0    0    0 ...    0  282    0  215   \n",
      "26    0    0   0    0   0     0    0    0    0    0 ...    0    0  289    0   \n",
      "27    0    0   0    0   0   214    0    0    0    0 ...    0    0    2    0   \n",
      "28    4    8   1    0  13   340    0    0    0    2 ...    0    0    5    0   \n",
      "29    0    0   0    0   2    34    0    0    0    0 ...    0    0    0    0   \n",
      "30   56    0   0    0   0     0    0    0    0    0 ...    0    0    0    0   \n",
      "31    0    0   0    0   0     0    0    0    0    0 ...    0    0    0    0   \n",
      "\n",
      "     26  27  28   29   30  31  \n",
      "0     0  19   0    0    1   0  \n",
      "1     0   0   0    0    0   0  \n",
      "2     0   0   0    0    0   0  \n",
      "3     0   0  67    7    0   0  \n",
      "4     0   4   1    1    0   0  \n",
      "5     0  14  43   17    0   0  \n",
      "6     0   0   7    6    0   0  \n",
      "7     0   0   0    0    0   0  \n",
      "8     0   9   0    0    0   0  \n",
      "9     0   0   1    0    0   0  \n",
      "10    0   0   6    4    0   0  \n",
      "11    0   0   0    0    0   0  \n",
      "12    0   3   0    0    0   0  \n",
      "13    0   6   3   17   43   0  \n",
      "14    0   2   0    0    0   0  \n",
      "15    0   0   0    0    0   0  \n",
      "16    0   0   0    0    0   0  \n",
      "17    0   0   0    0    0   0  \n",
      "18    0   0   0    0    0   0  \n",
      "19    0   0   0    0    0   0  \n",
      "20    0   0   0    0    0   0  \n",
      "21    0   0   0    2    0   0  \n",
      "22    0   0   0    0    0   0  \n",
      "23    0   0   0    0    0   0  \n",
      "24   39   0   0    0    0   0  \n",
      "25    0   0   0    0    0   0  \n",
      "26  213   0   0    0    0   0  \n",
      "27    6  71   0   44    0   0  \n",
      "28    0  13   0   88    0   0  \n",
      "29    0   0   0  711    0   0  \n",
      "30    0   3   0   10  971   0  \n",
      "31    0   0   0    0    0   0  \n",
      "\n",
      "[32 rows x 32 columns]\n",
      "F1 score ....... 0.700084245998\n",
      "\n",
      "Finished fitting model (leave out subject: 4) ....... 4\n",
      "Total time: 45.464059352874756\n",
      "------------------------------------------------------------\n",
      "Steps:  181 N imput 1\n",
      "(45731, 32)\n",
      "(19031, 32)\n",
      "Started fitting model (leave out subject: 5) ....... 5\n"
     ]
    }
   ],
   "source": [
    "#TODO -> add method calls and a for loop for leave one out\n",
    "\n",
    "temp = 0 # for printing the current subject\n",
    "num_epochs = 75 # for convenience\n",
    "results_acc = np.array([[]]).reshape(num_epochs,0)\n",
    "results_loss = np.array([[]]).reshape(num_epochs,0)\n",
    "results_f1 = []\n",
    "\n",
    "\n",
    "for index in df.index.unique():\n",
    "\n",
    "    #exception - subject does not contain all possible classes\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #get training and test sets\n",
    "\n",
    "    X_train, y_train, X_test, y_test = train_test(df, index)\n",
    "\n",
    "    training_data_count, test_data_count, n_steps, n_input = additional_parameters(X_train, X_test)\n",
    "    training_iters = training_data_count * training_iters\n",
    "\n",
    "    y_train = one_hot(y_train)\n",
    "    y_test = one_hot(y_test)\n",
    "    \n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "\n",
    "    #-------------------------------------------------------------\n",
    "    #make a model\n",
    "\n",
    "    input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(n_hidden, activation=\"relu\", name=\"fc1\")(x)\n",
    "    x = Dense(n_hidden, activation=\"relu\", name=\"fc2\")(x)\n",
    "    x = Dense(n_classes, activation=\"softmax\", name=\"prediction\")(x)\n",
    "    model = Model(input,x)\n",
    "    #model.summary()\n",
    "\n",
    "    #------------------------------------------------------------\n",
    "    #choose an activation function\n",
    "\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    #-------------------------------------------------------------\n",
    "    #fit a model\n",
    "\n",
    "    temp += 1\n",
    "    print('Started fitting model (leave out subject: ' + str(int(index)) + ') ....... ' + str(temp))\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    hist = model.fit(X_train, y_train, batch_size=batch_size, epochs = num_epochs, validation_data=(X_test, y_test), verbose=0)\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #gather results\n",
    "    \n",
    "    #Accuracy and loss\n",
    "    acc = np.array([hist.history['val_acc']]).reshape(len(hist.history['val_acc']),1)\n",
    "    loss = np.array([hist.history['val_loss']]).reshape(len(hist.history['val_loss']),1)\n",
    "    \n",
    "    results_acc = np.concatenate([results_acc, acc], axis = 1)\n",
    "    results_loss = np.concatenate([results_loss, loss], axis = 1)\n",
    "    \n",
    "    #Confusion matrix\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    conf = pd.DataFrame(metrics.confusion_matrix(y_true, y_pred, labels = class_array))\n",
    "    conf.to_csv(path + \"/Results/Basic regression network/Confidence_matrix_S\" + str(int(index)) + '_(' + str(sliding_window) + ',' + str(overlay) + \").csv\", index=False)\n",
    "    \n",
    "    #F1 score\n",
    "    results_f1.append(metrics.f1_score(y_true, y_pred, average='micro'))\n",
    "    \n",
    "    print('')\n",
    "    print('Confusion matrix:')\n",
    "    print(conf)\n",
    "    print('F1 score ....... ' + str(results_f1[temp-1]))\n",
    "    print('')\n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    print('Finished fitting model (leave out subject: ' + str(int(index)) + ') ....... ' + str(temp))\n",
    "    print(\"Total time: \" + str(t1-t0))\n",
    "    print('------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy ...... 0.595815266404\n",
      "Average loss .......... 1.37953065779\n",
      "Average F1 score ...... 0.0\n",
      "     0    1    2    3   4    5   6   7    8    9  ...  22  23  24  25  26  27  \\\n",
      "0   155    0    0    0   0    3   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "1     0  312    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "2     0    0  164    0   0  109   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "3     0    0    0  191   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "4     0    0    0    0  43   68   0   0   14    0 ...   0   0   0   0   0   0   \n",
      "5     0    0    5    0  23  759   0   0    0    0 ...   0   0   0   0   0   4   \n",
      "6    39    0    0    0   0    0  86   0    0    0 ...   0   0   0   0   0   0   \n",
      "7     0    0    0    0   0   24   0  42   59    0 ...   0   0   0   0   0   0   \n",
      "8     0    0    0    0   2   13   0   0  131    0 ...   0   0   0   0   0   0   \n",
      "9     0    0    0    3   2   25  17   0    0  268 ...   0   0   0   0   0   0   \n",
      "10    1    0    1    2  13  137  12   2    2   75 ...   0   0   0   0   0   3   \n",
      "11    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "12    0    0    0    0   0  104   0  28    6    0 ...   0   0   0   0   0   0   \n",
      "13    0    0    0    0  12    2   0   0   29    0 ...   0   0   0   0   0   0   \n",
      "14    0    0    0    0   0    0   0  97    0    0 ...   0   0   0   0   0   0   \n",
      "15    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "16    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "17    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "18    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "19    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "20    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "21    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "22    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "23    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "24    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "25    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "26    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "27    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "28    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "29    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "30    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "31    0    0    0    0   0    0   0   0    0    0 ...   0   0   0   0   0   0   \n",
      "\n",
      "    28  29  30  31  \n",
      "0    0   0   0   0  \n",
      "1    0   0   0   0  \n",
      "2    0   0   0   0  \n",
      "3    0   0   0   0  \n",
      "4    0   0   0   0  \n",
      "5    1  14   0   0  \n",
      "6    0   0   0   0  \n",
      "7    0   0   0   0  \n",
      "8    0   0   0   0  \n",
      "9    0   0   0   0  \n",
      "10   9   2   0   0  \n",
      "11   0   0   0   0  \n",
      "12   0   0   0   0  \n",
      "13   0   9   0   0  \n",
      "14   0   0   0   0  \n",
      "15   0   0   0   0  \n",
      "16   0   0   0   0  \n",
      "17   0   0   0   0  \n",
      "18   0   0   0   0  \n",
      "19   0   0   0   0  \n",
      "20   0   0   0   0  \n",
      "21   0   0   0   0  \n",
      "22   0   0   0   0  \n",
      "23   0   0   0   0  \n",
      "24   0   0   0   0  \n",
      "25   0   0   0   0  \n",
      "26   0   0   0   0  \n",
      "27   0   0   0   0  \n",
      "28   0   0   0   0  \n",
      "29   0   0   0   0  \n",
      "30   0   0   0   0  \n",
      "31   0   0   0   0  \n",
      "\n",
      "[32 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "#print some results\n",
    "print('Average accuracy ...... ' + str( np.sum(np.sum(results_acc, axis=0)/num_epochs)/temp) )\n",
    "print('Average loss .......... ' + str( np.sum(np.sum(results_loss, axis=0)/num_epochs)/temp) )\n",
    "print('Average F1 score ...... ' + str( sum(results_f1)/temp) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#add some labels and save the results to csv\n",
    "\n",
    "#Accuracy and loss:\n",
    "result_acc = pd.DataFrame(results_acc, columns = df.index.unique())\n",
    "result_loss = pd.DataFrame(results_loss, columns = df.index.unique())\n",
    "\n",
    "result_acc.to_csv(path + \"/Results/Basic regression network/Accuracy\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)\n",
    "result_loss.to_csv(path + \"/Results/Basic regression network/Loss\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)\n",
    "\n",
    "#F1 score:\n",
    "result_f1 = pd.DataFrame(np.array([results_f1]), columns = df.index.unique().values.astype(int))\n",
    "result_f1.to_csv(path + \"/Results/Basic regression network/F1\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:  1000 N imput 1\n",
      "(25277, 13)\n",
      "(1456, 13)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 1000, 1)           0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 1000, 32)          64        \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000, 32)          8320      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 13)                429       \n",
      "=================================================================\n",
      "Total params: 17,133\n",
      "Trainable params: 17,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Started fitting model (leave out subject: 1.0) ....... 1\n",
      "Train on 25277 samples, validate on 1456 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "temp = 0 # for printing the current subject\n",
    "num_epochs = 2 # for convenience\n",
    "results_acc = np.array([[]]).reshape(num_epochs,0)\n",
    "results_loss = np.array([[]]).reshape(num_epochs,0)\n",
    "results_f1 = []\n",
    "\n",
    "for index in df.index.unique():\n",
    "\n",
    "    #exception - subject does not contain all possible classes\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #get training and test sets\n",
    "\n",
    "    X_train, y_train, X_test, y_test = train_test(df, index)\n",
    "\n",
    "    training_data_count, test_data_count, n_steps, n_input = additional_parameters(X_train, X_test)\n",
    "    training_iters = training_data_count * training_iters\n",
    "\n",
    "    y_train = one_hot(y_train)\n",
    "    y_test = one_hot(y_test)\n",
    "    \n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "\n",
    "    #-------------------------------------------------------------\n",
    "    #make a model\n",
    "    \n",
    "    input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "    x = Dense(n_hidden, activation=\"relu\", name=\"fc1\")(input)\n",
    "    x = LSTM(n_hidden, return_sequences=True, name='lstm_1')(x)\n",
    "    x = LSTM(n_hidden, name='lstm_2')(x)\n",
    "    x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "    model = Model(input,x)\n",
    "    model.summary()\n",
    "    \n",
    "    #------------------------------------------------------------\n",
    "    #choose an activation function\n",
    "\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #train a model\n",
    "\n",
    "    temp += 1\n",
    "    print('Started fitting model (leave out subject: ' + str(index) + ') ....... ' + str(temp))\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    hist = model.fit(X_train, y_train, batch_size=batch_size, epochs = num_epochs, validation_data=(X_test, y_test), verbose=0) \n",
    "\n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #gather results\n",
    "    \n",
    "    #Accuracy and loss\n",
    "    acc = np.array([hist.history['val_acc']]).reshape(len(hist.history['val_acc']),1)\n",
    "    loss = np.array([hist.history['val_loss']]).reshape(len(hist.history['val_loss']),1)\n",
    "    \n",
    "    results_acc = np.concatenate([results_acc, acc], axis = 1)\n",
    "    results_loss = np.concatenate([results_loss, loss], axis = 1)\n",
    "    \n",
    "    #Confusion matrix\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    conf = pd.DataFrame(metrics.confusion_matrix(y_true, y_pred, labels = class_array))\n",
    "    conf.to_csv(path + \"/Results/Basic regression network/Confidence_matrix_S\" + str(int(index)) + '_(' + str(sliding_window) + ',' + str(overlay) + \").csv\", index=False)\n",
    "    \n",
    "    #F1 score\n",
    "    results_f1.append(metrics.f1_score(y_true, y_pred, average='micro'))\n",
    "    \n",
    "    print('')\n",
    "    print('Confusion matrix:')\n",
    "    print(conf)\n",
    "    print('F1 score ....... ' + str(results_f1[temp-1]))\n",
    "    print('')\n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    print('Finished fitting model (leave out subject: ' + str(int(index)) + ') ....... ' + str(temp))\n",
    "    print(\"Total time: \" + str(t1-t0))\n",
    "    print('------------------------------------------------------------')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print some results\n",
    "print('Average accuracy ...... ' + str( np.sum(np.sum(results_acc, axis=0)/num_epochs)/temp) )\n",
    "print('Average loss .......... ' + str( np.sum(np.sum(results_loss, axis=0)/num_epochs)/temp) )\n",
    "print('Average F1 score ...... ' + str( sum(results_f1)/temp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add some labels and save the results to csv\n",
    "\n",
    "#Accuracy and loss:\n",
    "result_acc = pd.DataFrame(results_acc, columns = df.index.unique())\n",
    "result_loss = pd.DataFrame(results_loss, columns = df.index.unique())\n",
    "\n",
    "result_acc.to_csv(path + \"/Results/Basic LSTM network/Accuracy\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)\n",
    "result_loss.to_csv(path + \"/Results/Basic LSTM network/Loss\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)\n",
    "\n",
    "#F1 score:\n",
    "result_f1 = pd.DataFrame(np.array([results_f1]), columns = df.index.unique().values.astype(int))\n",
    "result_f1.to_csv(path + \"/Results/Basic LSTM network/F1\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic bidirectional stacked LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 1000, 1)           0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 1000, 32)          64        \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1000, 64)          16640     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 41,666\n",
      "Trainable params: 41,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "temp = 0 # for printing the current subject\n",
    "num_epochs = 2 # for convenience\n",
    "results_acc = np.array([[]]).reshape(num_epochs,0)\n",
    "results_loss = np.array([[]]).reshape(num_epochs,0)\n",
    "results_f1 = []\n",
    "\n",
    "\n",
    "for index in df.index.unique():\n",
    "\n",
    "    #exception - subject does not contain all possible classes\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #get training and test sets\n",
    "\n",
    "    X_train, y_train, X_test, y_test = train_test(df, index)\n",
    "\n",
    "    training_data_count, test_data_count, n_steps, n_input = additional_parameters(X_train, X_test)\n",
    "    training_iters = training_data_count * training_iters\n",
    "\n",
    "    y_train = one_hot(y_train)\n",
    "    y_test = one_hot(y_test)\n",
    "    \n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "\n",
    "    #-------------------------------------------------------------\n",
    "    #make a model\n",
    "    \n",
    "    input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "    x = Dense(n_hidden, activation=\"relu\", name=\"fc1\")(input)\n",
    "    x = Bidirectional(LSTM(n_hidden, return_sequences=True, name='lstm_1'))(x)\n",
    "    x = Bidirectional(LSTM(n_hidden, name='lstm_2'))(x)\n",
    "    x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "    model = Model(input,x)\n",
    "    model.summary()\n",
    "    \n",
    "       \n",
    "    #------------------------------------------------------------\n",
    "    #choose an activation function\n",
    "\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #train a model\n",
    "    \n",
    "    temp += 1\n",
    "    print('Started fitting model (leave out subject: ' + str(index) + ') ....... ' + str(temp))\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    hist = model.fit(X_train, y_train, batch_size=batch_size, epochs = num_epochs, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #gather results\n",
    "    \n",
    "    #Accuracy and loss\n",
    "    acc = np.array([hist.history['val_acc']]).reshape(len(hist.history['val_acc']),1)\n",
    "    loss = np.array([hist.history['val_loss']]).reshape(len(hist.history['val_loss']),1)\n",
    "    \n",
    "    results_acc = np.concatenate([results_acc, acc], axis = 1)\n",
    "    results_loss = np.concatenate([results_loss, loss], axis = 1)\n",
    "    \n",
    "    #Confusion matrix\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    conf = pd.DataFrame(metrics.confusion_matrix(y_true, y_pred, labels = class_array))\n",
    "    conf.to_csv(path + \"/Results/Basic regression network/Confidence_matrix_S\" + str(int(index)) + '_(' + str(sliding_window) + ',' + str(overlay) + \").csv\", index=False)\n",
    "    \n",
    "    #F1 score\n",
    "    results_f1.append(metrics.f1_score(y_true, y_pred, average='micro'))\n",
    "    \n",
    "    print('')\n",
    "    print('Confusion matrix:')\n",
    "    print(conf)\n",
    "    print('F1 score ....... ' + str(results_f1[temp-1]))\n",
    "    print('')\n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    print('Finished fitting model (leave out subject: ' + str(int(index)) + ') ....... ' + str(temp))\n",
    "    print(\"Total time: \" + str(t1-t0))\n",
    "    print('------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print some results\n",
    "print('Average accuracy ...... ' + str( np.sum(np.sum(results_acc, axis=0)/num_epochs)/temp) )\n",
    "print('Average loss .......... ' + str( np.sum(np.sum(results_loss, axis=0)/num_epochs)/temp) )\n",
    "print('Average F1 score ...... ' + str( sum(results_f1)/temp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add some labels and save the results to csv\n",
    "\n",
    "#Accuracy and loss:\n",
    "result_acc = pd.DataFrame(results_acc, columns = df.index.unique())\n",
    "result_loss = pd.DataFrame(results_loss, columns = df.index.unique())\n",
    "\n",
    "result_acc.to_csv(path + \"/Results/Basic bidirectional stacked LSTMs/Accuracy\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)\n",
    "result_loss.to_csv(path + \"/Results/Basic bidirectional stacked LSTMs/Loss\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)\n",
    "\n",
    "#F1 score:\n",
    "result_f1 = pd.DataFrame(np.array([results_f1]), columns = df.index.unique().values.astype(int))\n",
    "result_f1.to_csv(path + \"/Results/Basic bidirectional stacked LSTMs/F1\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3x3 HAR stacked residual bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "lambda_loss_amount = 0.005\n",
    "clip_gradients = 15.0\n",
    "n_layers_in_highway = 0\n",
    "n_stacked_layers = 3\n",
    "n_hidden = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input (InputLayer)               (None, 1000, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "fc1 (Dense)                      (None, 1000, 32)      64          input[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "bi_lstm1 (Bidirectional)         (None, 1000, 32)      6272        fc1[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "fc2 (Dense)                      (None, 1000, 32)      1056        bi_lstm1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bi_lstm2 (Bidirectional)         (None, 1000, 32)      6272        fc2[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "fc3 (Dense)                      (None, 1000, 32)      1056        bi_lstm2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 1000, 32)      0           fc2[0][0]                        \n",
      "                                                                   fc3[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "bi_lstm3 (Bidirectional)         (None, 1000, 32)      6272        add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "fc4 (Dense)                      (None, 1000, 32)      1056        bi_lstm3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bi_lstm4 (Bidirectional)         (None, 1000, 32)      6272        fc4[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "fc5 (Dense)                      (None, 1000, 32)      1056        bi_lstm4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 1000, 32)      0           fc4[0][0]                        \n",
      "                                                                   fc5[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "bi_lstm5 (Bidirectional)         (None, 1000, 32)      6272        add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "fc6 (Dense)                      (None, 1000, 32)      1056        bi_lstm5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bi_lstm6 (Bidirectional)         (None, 1000, 32)      6272        fc6[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "fc7 (Dense)                      (None, 1000, 32)      1056        bi_lstm6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 1000, 32)      0           fc6[0][0]                        \n",
      "                                                                   fc7[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 32000)         0           add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "prediction (Dense)               (None, 2)             64002       flatten_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 108,034\n",
      "Trainable params: 108,034\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "temp = 0 # for printing the current subject\n",
    "num_epochs = 2 # on last layer\n",
    "results_acc = np.array([[]]).reshape(num_epochs,0)\n",
    "results_loss = np.array([[]]).reshape(num_epochs,0)\n",
    "results_f1 = []\n",
    "\n",
    "for index in df.index.unique():\n",
    "\n",
    "    #exception - subject does not contain all possible classes\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #get training and test sets\n",
    "\n",
    "    X_train, y_train, X_test, y_test = train_test(df, index)\n",
    "\n",
    "    training_data_count, test_data_count, n_steps, n_input = additional_parameters(X_train, X_test)\n",
    "    training_iters = training_data_count * training_iters\n",
    "\n",
    "    y_train = one_hot(y_train)\n",
    "    y_test = one_hot(y_test)\n",
    "    \n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "\n",
    "    ################  FIRST  ##################\n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #make a model\n",
    "    \n",
    "    input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "    x = Dense(32, activation=\"relu\", name=\"fc1\")(input)\n",
    "    x = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm1')(x)\n",
    "    x = Dense(32, activation=\"relu\", name=\"fc2\")(x)\n",
    "    shortcut = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm2')(x)\n",
    "    shortcut = Dense(32, activation=\"relu\", name=\"fc3\")(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    # x = BatchNormalization(name='bn1')(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm3')(x)\n",
    "    x = Dense(32, activation=\"relu\", name=\"fc4\")(x)\n",
    "    shortcut = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm4')(x)\n",
    "    shortcut = Dense(32, activation=\"relu\", name=\"fc5\")(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    # x = BatchNormalization(name='bn2')(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm5')(x)\n",
    "    x = Dense(32, activation=\"relu\", name=\"fc6\")(x)\n",
    "    shortcut = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm6')(x)\n",
    "    shortcut = Dense(32, activation=\"relu\", name=\"fc7\")(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    # x = BatchNormalization(name='bn2')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "\n",
    "    model = Model(input,x)\n",
    "    model.summary()\n",
    "    \n",
    "        \n",
    "       \n",
    "    #------------------------------------------------------------\n",
    "    #choose an activation function\n",
    "\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #train a model\n",
    "    \n",
    "    temp += 1\n",
    "    print('Started fitting model (leave out subject: ' + str(index) + ') ....... ' + str(temp))\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs = 20, validation_data=(X_test, y_test), verbose=0)\n",
    "    #results.append(model.evaluate(X_test,y_test))\n",
    "    print('')\n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    print('Finished fitting model (leave out subject: ' + str(index) + ') ....... ' + str(temp))\n",
    "    print(\"Total time: \" + str(t1-t0))\n",
    "    print('------------------------------------------------------------')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ############## Second ##########################\n",
    "\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #make a model\n",
    "    \n",
    "    input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "    x = Dense(32, activation=\"relu\", name=\"fc1\")(input)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True), name='bi_lstm1')(x)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True), name='bi_lstm2')(x)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True), name='bi_lstm3')(x)\n",
    "    x = Dense(32, activation=\"relu\", name=\"fc2\")(x)\n",
    "    # x = BatchNormalization(name='bn1')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "\n",
    "    model = Model(input,x)\n",
    "    model.summary()\n",
    "       \n",
    "    #------------------------------------------------------------\n",
    "    #choose an activation function\n",
    "\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #train a model\n",
    "    \n",
    "    temp += 1\n",
    "    print('Started fitting model (leave out subject: ' + str(index) + ') ....... ' + str(temp))\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs = 200, validation_data=(X_test, y_test), verbose=0)\n",
    "    #results.append(model.evaluate(X_test,y_test))\n",
    "    print('')\n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    print('Finished fitting model (leave out subject: ' + str(index) + ') ....... ' + str(temp))\n",
    "    print(\"Total time: \" + str(t1-t0))\n",
    "    print('------------------------------------------------------------')\n",
    "    \n",
    "    \n",
    "    ################ Third ############################\n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #make a model\n",
    "    \n",
    "    input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "    x = Dense(32, activation=\"relu\", name=\"fc1\")(input)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True), name='bi_lstm1')(x)\n",
    "    # x = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm2')(x)\n",
    "    x = Dense(32, activation=\"relu\", name=\"fc4\")(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "\n",
    "    model = Model(input,x)\n",
    "    model.summary()\n",
    "       \n",
    "    #------------------------------------------------------------\n",
    "    #choose an activation function\n",
    "\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #train a model\n",
    "    \n",
    "    temp += 1\n",
    "    print('Started fitting model (leave out subject: ' + str(index) + ') ....... ' + str(temp))\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    hist = model.fit(X_train, y_train, batch_size=batch_size, epochs = num_epochs, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #gather results\n",
    "    \n",
    "    #Accuracy and loss\n",
    "    acc = np.array([hist.history['val_acc']]).reshape(len(hist.history['val_acc']),1)\n",
    "    loss = np.array([hist.history['val_loss']]).reshape(len(hist.history['val_loss']),1)\n",
    "    \n",
    "    results_acc = np.concatenate([results_acc, acc], axis = 1)\n",
    "    results_loss = np.concatenate([results_loss, loss], axis = 1)\n",
    "    \n",
    "    #Confusion matrix\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    conf = pd.DataFrame(metrics.confusion_matrix(y_true, y_pred, labels = class_array))\n",
    "    conf.to_csv(path + \"/Results/Basic regression network/Confidence_matrix_S\" + str(int(index)) + '_(' + str(sliding_window) + ',' + str(overlay) + \").csv\", index=False)\n",
    "    \n",
    "    #F1 score\n",
    "    results_f1.append(metrics.f1_score(y_true, y_pred, average='micro'))\n",
    "    \n",
    "    print('')\n",
    "    print('Confusion matrix:')\n",
    "    print(conf)\n",
    "    print('F1 score ....... ' + str(results_f1[temp-1]))\n",
    "    print('')\n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    print('Finished fitting model (leave out subject: ' + str(int(index)) + ') ....... ' + str(temp))\n",
    "    print(\"Total time: \" + str(t1-t0))\n",
    "    print('------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print some results\n",
    "print('Average accuracy ...... ' + str( np.sum(np.sum(results_acc, axis=0)/num_epochs)/temp) )\n",
    "print('Average loss .......... ' + str( np.sum(np.sum(results_loss, axis=0)/num_epochs)/temp) )\n",
    "print('Average F1 score ...... ' + str( sum(results_f1)/temp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add some labels and save the results to csv\n",
    "\n",
    "#Accuracy and loss:\n",
    "result_acc = pd.DataFrame(results_acc, columns = df.index.unique())\n",
    "result_loss = pd.DataFrame(results_loss, columns = df.index.unique())\n",
    "\n",
    "result_acc.to_csv(path + \"/Results/3x3 HAR stacked residual bidirectional LSTMs/Accuracy\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)\n",
    "result_loss.to_csv(path + \"/Results/3x3 HAR stacked residual bidirectional LSTMs/Loss\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)\n",
    "\n",
    "#F1 score:\n",
    "result_f1 = pd.DataFrame(np.array([results_f1]), columns = df.index.unique().values.astype(int))\n",
    "result_f1.to_csv(path + \"/Results/3x3 HAR stacked residual bidirectional LSTMs/F1\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepConvLSTM\n",
    "<br>\n",
    "This Part of the code only does a summary of the model, the rest is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "x = Dense(128, activation=\"relu\", name=\"fc1\")(input)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True), name='bi_lstm1')(x)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True), name='bi_lstm2')(x)\n",
    "x = Dense(128, activation=\"relu\", name=\"fc4\")(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "\n",
    "model = Model(input,x)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:  72900 N imput 1\n",
      "(296, 5)\n",
      "(195, 5)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 72900, 1)          0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 72896, 64)         384       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 36448, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv1D)               (None, 36444, 64)         20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 18222, 64)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 1166208)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1166208)           0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 5)                 5831045   \n",
      "=================================================================\n",
      "Total params: 5,851,973\n",
      "Trainable params: 5,851,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Started fitting model (leave out subject: 2.0) ....... 1\n"
     ]
    }
   ],
   "source": [
    "temp = 0 # for printing the current subject\n",
    "num_epochs = 2 # for convenience\n",
    "results_acc = np.array([[]]).reshape(num_epochs,0)\n",
    "results_loss = np.array([[]]).reshape(num_epochs,0)\n",
    "results_f1 = []\n",
    "\n",
    "for index in df.index.unique():\n",
    "\n",
    "    #exception - subject does not contain all possible classes\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #get training and test sets\n",
    "\n",
    "    X_train, y_train, X_test, y_test = train_test(df, index)\n",
    "\n",
    "    training_data_count, test_data_count, n_steps, n_input = additional_parameters(X_train, X_test)\n",
    "    training_iters = training_data_count * training_iters\n",
    "\n",
    "    y_train = one_hot(y_train)\n",
    "    y_test = one_hot(y_test)\n",
    "    \n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #make a model\n",
    "\n",
    "    input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "    x = Conv1D(64, 5, activation='relu', name='conv1')(input)\n",
    "    x = MaxPool1D()(x)\n",
    "    x = Conv1D(64, 5, activation='relu', name='conv2')(x)\n",
    "    x = MaxPool1D()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(.5)(x)\n",
    "    x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "\n",
    "    model = Model(input,x)\n",
    "    model.summary()\n",
    "    \n",
    "       \n",
    "    #------------------------------------------------------------\n",
    "    #choose an activation function\n",
    "\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #train a model\n",
    "    \n",
    "    temp += 1\n",
    "    print('Started fitting model (leave out subject: ' + str(index) + ') ....... ' + str(temp))\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    hist = model.fit(X_train, y_train, batch_size=batch_size, epochs = num_epochs, validation_data=(X_test, y_test), verbose=0)\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #gather results\n",
    "    \n",
    "    #Accuracy and loss\n",
    "    acc = np.array([hist.history['val_acc']]).reshape(len(hist.history['val_acc']),1)\n",
    "    loss = np.array([hist.history['val_loss']]).reshape(len(hist.history['val_loss']),1)\n",
    "    \n",
    "    results_acc = np.concatenate([results_acc, acc], axis = 1)\n",
    "    results_loss = np.concatenate([results_loss, loss], axis = 1)\n",
    "    \n",
    "    #Confusion matrix\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    conf = pd.DataFrame(metrics.confusion_matrix(y_true, y_pred, labels = class_array))\n",
    "    conf.to_csv(path + \"/Results/Basic ConvLSTM/Confidence_matrix_S\" + str(int(index)) + '_(' + str(sliding_window) + ',' + str(overlay) + \").csv\", index=False)\n",
    "    \n",
    "    #F1 score\n",
    "    results_f1.append(metrics.f1_score(y_true, y_pred, average='micro'))\n",
    "    \n",
    "    print('')\n",
    "    print('Confusion matrix:')\n",
    "    print(conf)\n",
    "    print('F1 score ....... ' + str(results_f1[temp-1]))\n",
    "    print('')\n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    print('Finished fitting model (leave out subject: ' + str(int(index)) + ') ....... ' + str(temp))\n",
    "    print(\"Total time: \" + str(t1-t0))\n",
    "    print('------------------------------------------------------------')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy ...... 0.695637532839\n",
      "Average loss .......... 1.80666267441\n",
      "Average F1 score ...... 0.731845238095\n"
     ]
    }
   ],
   "source": [
    "#print some results\n",
    "\n",
    "print('Average accuracy ...... ' + str( np.sum(np.sum(results_acc, axis=0)/num_epochs)/temp) )\n",
    "print('Average loss .......... ' + str( np.sum(np.sum(results_loss, axis=0)/num_epochs)/temp) )\n",
    "print('Average F1 score ...... ' + str( sum(results_f1)/temp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add some labels and save the results to csv\n",
    "\n",
    "#Accuracy and loss:\n",
    "result_acc = pd.DataFrame(results_acc, columns = df.index.unique())\n",
    "result_loss = pd.DataFrame(results_loss, columns = df.index.unique())\n",
    "\n",
    "result_acc.to_csv(path + \"/Results/Basic ConvLSTM/Accuracy\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)\n",
    "result_loss.to_csv(path + \"/Results/Basic ConvLSTM/Loss\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)\n",
    "\n",
    "#F1 score:\n",
    "result_f1 = pd.DataFrame(np.array([results_f1]), columns = df.index.unique().values.astype(int))\n",
    "result_f1.to_csv(path + \"/Results/Basic ConvLSTM/F1\" + \"_(\" + str(sliding_window) + \",\" + str(overlay) + \").csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
