{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs for Human Activity Recognition\n",
    "\n",
    "Human activity recognition using smartphones dataset and an LSTM RNN. Classifying the type of movement amongst six categories:\n",
    "- WALKING,\n",
    "- WALKING_UPSTAIRS,\n",
    "- WALKING_DOWNSTAIRS,\n",
    "- SITTING,\n",
    "- STANDING,\n",
    "- LAYING.\n",
    "\n",
    "Compared to a classical approach, using a Recurrent Neural Networks (RNN) with Long Short-Term Memory cells (LSTMs) require no or almost no feature engineering. Data can be fed directly into the neural network who acts like a black box, modeling the problem correctly. Other research on the activity recognition dataset used mostly use a big amount of feature engineering, which is rather a signal processing approach combined with classical data science techniques. The approach here is rather very simple in terms of how much did the data was preprocessed. \n",
    "\n",
    "## Video dataset overview\n",
    "\n",
    "Follow this link to see a video of the 6 activities recorded in the experiment with one of the participants:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=XOEN9W05_4A\n",
    "\" target=\"_blank\"><img src=\"http://img.youtube.com/vi/XOEN9W05_4A/0.jpg\" \n",
    "alt=\"Video of the experiment\" width=\"400\" height=\"300\" border=\"10\" /></a>\n",
    "  <a href=\"https://youtu.be/XOEN9W05_4A\"><center>[Watch video]</center></a>\n",
    "</p>\n",
    "\n",
    "## Details about input data\n",
    "\n",
    "I will be using an LSTM on the data to learn (as a cellphone attached on the waist) to recognise the type of activity that the user is doing. The dataset's description goes like this:\n",
    "\n",
    "> The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal, which has gravitational and body motion components, was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components, therefore a filter with 0.3 Hz cutoff frequency was used. \n",
    "\n",
    "That said, I will use the almost raw data: only the gravity effect has been filtered out of the accelerometer  as a preprocessing step for another 3D feature as an input to help learning. \n",
    "\n",
    "## What is an RNN?\n",
    "\n",
    "As explained in [this article](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), an RNN takes many input vectors to process them and output other vectors. It can be roughly pictured like in the image below, imagining each rectangle has a vectorial depth and other special hidden quirks in the image below. **In our case, the \"many to one\" architecture is used**: we accept time series of feature vectors (one vector per time step) to convert them to a probability vector at the output for classification. Note that a \"one to one\" architecture would be a standard feedforward neural network. \n",
    "\n",
    "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" />\n",
    "\n",
    "An LSTM is an improved RNN. It is more complex, but easier to train, avoiding what is called the vanishing gradient problem. \n",
    "\n",
    "\n",
    "## Results \n",
    "\n",
    "Scroll on! Nice visuals awaits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#loader class for loading and preprocessing the data sets\n",
    "from loader import Loader\n",
    "import time\n",
    "\n",
    "# All Includes\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras import layers\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Reshape, Dropout, Conv1D, MaxPool1D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.wrappers import TimeDistributed, Bidirectional\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "\n",
    "np.random.seed(1337)\n",
    "tf.set_random_seed(1339)\n",
    "\n",
    "session_conf = tf.ConfigProto(log_device_placement=True)\n",
    "# # Avoid full memory allocation on GPU\n",
    "session_conf.gpu_options.allow_growth=True\n",
    "\n",
    "sess = tf.InteractiveSession(config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a dataset and preparing it:\n",
    "\n",
    "1) Run the appropriate loader from Loader  class <br>\n",
    "2) Run the appropriate preprocessor (sliding window size , overlay) <br>\n",
    "3) Load the pickled output file of the loader <br>\n",
    "\n",
    "Could potentially be used in a loop for automated testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dis/.local/lib/python3.5/site-packages/ipykernel_launcher.py:16: DtypeWarning: Columns (5,6,7,8,9,10,17,18,19,26,27,28,38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1\n",
      "Columns shape: (1, 40)\n",
      "Data shape:(23800, 40)\n",
      "(23800, 39)\n",
      "-------------------------------------------------------------------\n",
      "Subject 2\n",
      "Columns shape: (1, 40)\n",
      "Data shape:(64389, 40)\n",
      "(64389, 39)\n",
      "-------------------------------------------------------------------\n",
      "Subject 3\n",
      "Columns shape: (1, 40)\n",
      "Data shape:(33856, 40)\n",
      "(33856, 39)\n",
      "-------------------------------------------------------------------\n",
      "Subject 4\n",
      "Columns shape: (1, 40)\n",
      "Data shape:(62194, 40)\n",
      "(62194, 39)\n",
      "-------------------------------------------------------------------\n",
      "Subject 5\n",
      "Columns shape: (1, 40)\n",
      "Data shape:(76607, 40)\n",
      "(76607, 39)\n",
      "-------------------------------------------------------------------\n",
      "(260846, 39)\n"
     ]
    }
   ],
   "source": [
    "load = Loader()\n",
    "\n",
    "\n",
    "#load.ward1_load()\n",
    "#load.ward1_preprocess(40,20)\n",
    "#load.opportunity_load()\n",
    "#load.opportunity_preprocess(300,80)\n",
    "#load.daily_sports_load()\n",
    "#load.daily_sports_preprocess(25,15)\n",
    "#load.pamap2_load()\n",
    "#load.pamap2_preprocess(270, 30)\n",
    "\n",
    "#load.chiron_first_load()\n",
    "#frame = load.chiron_first_preprocess(10, 5)\n",
    "#load.chiron_second_load()\n",
    "frame = load.chiron_second_preprocess(1, 0)\n",
    "\n",
    "\n",
    "\n",
    "#df = pickle.load(open('WARD1/processed_data.txt','rb')) # -> works\n",
    "#df = pickle.load(open('OpportunityUCIDataset/full_preprocessed_dataset.txt','rb')) # -> works\n",
    "#df = pickle.load(open('Daily and sports activities/full_preprocessed.txt','rb')) -> works\n",
    "#df = pickle.load(open('PAMAP2/full_preprocessed.txt','rb'))\n",
    "\n",
    "#df = pickle.load(open('Chiron first/Chiron_full_preprocessed.txt' ,'rb'))\n",
    "df = pickle.load(open('Chiron second/Chiron_full_preprocessed.txt' ,'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the leave one out method:\n",
    "\n",
    "Using methods that prepare the next leave one out division <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df.values[:,0])\n",
    "\n",
    "classes = len(encoder.classes_)\n",
    "\n",
    "def train_test(data, index):\n",
    "    #training and test sets -> leave one out\n",
    "    leave_out = df.index.isin([index])\n",
    "\n",
    "    X_train = df[~leave_out].values[:,2:]\n",
    "    y_train = df[~leave_out].values[:,0]\n",
    "\n",
    "    X_test = df[leave_out].values[:,2:]\n",
    "    y_test = df[leave_out].values[:,0]\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "\n",
    "    #normalization\n",
    "    X_test = normalize(X_test)\n",
    "    X_train = normalize(X_train)\n",
    "    \n",
    "    #--------------------------------------------------------------\n",
    "    \n",
    "    #reshaping to fit keras\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additionnal Parameters:\n",
    "\n",
    "Here are some core parameter definitions for the training. \n",
    "\n",
    "The whole neural network's structure could be summarised by enumerating those parameters and the fact an LSTM is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def additional_parameters(X_train, X_test):\n",
    "\n",
    "    training_data_count = X_train.shape[0]  # 21522 training series (with 50% overlap between each serie)\n",
    "    test_data_count = X_test.shape[0]  # 7794 testing series\n",
    "\n",
    "    n_steps = X_train.shape[1]  # 128 timesteps per series\n",
    "    n_input = len(X_train[0][0])  # 7 input parameters per timestep\n",
    "\n",
    "    print(\"Steps: \",n_steps, \"N imput\",n_input)\n",
    "    \n",
    "    return training_data_count, test_data_count, n_steps, n_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\n"
     ]
    }
   ],
   "source": [
    "# Input Data \n",
    "\n",
    "# LSTM Neural Network's internal structure\n",
    "\n",
    "n_hidden = 32 # Hidden layer num of features\n",
    "n_classes = classes # Total classes (should go up, or should go down)\n",
    "\n",
    "# Training \n",
    "\n",
    "learning_rate = 0.0025\n",
    "lambda_loss_amount = 0.0015\n",
    "training_iters = 300  # Loop 300 times on the dataset\n",
    "batch_size = 1500\n",
    "display_iter = 30000  # To show test set accuracy during training\n",
    "\n",
    "\n",
    "# Some debugging info\n",
    "\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "#print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test)) # -> takes too long\n",
    "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(y_):\n",
    "    #encoder = LabelEncoder()\n",
    "    y_encoded = encoder.transform(y_.ravel())#fit_transform(y_.ravel())\n",
    "    return np_utils.to_categorical(y_encoded, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic regression network\n",
    "\n",
    "1) Get training and test data sets and calculate some additional parameters <br>\n",
    "2) Use the additional parameters to build a model <br>\n",
    "3) Train, test and evaluate the model <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps:  37 N imput 1\n",
      "(237046, 32)\n",
      "(23800, 32)\n",
      "Started fitting model (leave out subject: 1.0) ....... 1\n",
      "22784/23800 [===========================>..] - ETA: 0s\n",
      "Finished fitting model (leave out subject: 1.0) ....... 1\n",
      "Total time: 142.20971179008484\n",
      "------------------------------------------------------------\n",
      "Steps:  37 N imput 1\n",
      "(196457, 32)\n",
      "(64389, 32)\n",
      "Started fitting model (leave out subject: 2.0) ....... 2\n",
      "63360/64389 [============================>.] - ETA: 0s\n",
      "Finished fitting model (leave out subject: 2.0) ....... 2\n",
      "Total time: 130.0432460308075\n",
      "------------------------------------------------------------\n",
      "Steps:  37 N imput 1\n",
      "(226990, 32)\n",
      "(33856, 32)\n",
      "Started fitting model (leave out subject: 3.0) ....... 3\n",
      "33248/33856 [============================>.] - ETA: 0s\n",
      "Finished fitting model (leave out subject: 3.0) ....... 3\n",
      "Total time: 139.90295243263245\n",
      "------------------------------------------------------------\n",
      "Steps:  37 N imput 1\n",
      "(198652, 32)\n",
      "(62194, 32)\n",
      "Started fitting model (leave out subject: 4.0) ....... 4\n",
      "61248/62194 [============================>.] - ETA: 0s\n",
      "Finished fitting model (leave out subject: 4.0) ....... 4\n",
      "Total time: 132.7987084388733\n",
      "------------------------------------------------------------\n",
      "Steps:  37 N imput 1\n",
      "(184239, 32)\n",
      "(76607, 32)\n",
      "Started fitting model (leave out subject: 5.0) ....... 5\n",
      "75968/76607 [============================>.] - ETA: 0s\n",
      "Finished fitting model (leave out subject: 5.0) ....... 5\n",
      "Total time: 128.3276870250702\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#TODO -> add method calls and a for loop for leave one out\n",
    "\n",
    "temp = 0\n",
    "results = []\n",
    "\n",
    "for index in df.index.unique():\n",
    "\n",
    "    #exception - subject does not contain all possible classes\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------------\n",
    "    #get training and test sets\n",
    "\n",
    "    X_train, y_train, X_test, y_test = train_test(df, index)\n",
    "\n",
    "    training_data_count, test_data_count, n_steps, n_input = additional_parameters(X_train, X_test)\n",
    "    training_iters = training_data_count * training_iters\n",
    "\n",
    "    y_train = one_hot(y_train)\n",
    "    y_test = one_hot(y_test)\n",
    "    \n",
    "    print(y_train.shape)\n",
    "    print(y_test.shape)\n",
    "\n",
    "\n",
    "    #-------------------------------------------------------------\n",
    "    #make a model\n",
    "\n",
    "    input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "    x = Flatten()(input)\n",
    "    x = Dense(n_hidden, activation=\"relu\", name=\"fc1\")(x)\n",
    "    x = Dense(n_hidden, activation=\"relu\", name=\"fc2\")(x)\n",
    "    x = Dense(n_classes, activation=\"softmax\", name=\"prediction\")(x)\n",
    "    model = Model(input,x)\n",
    "    #model.summary()\n",
    "\n",
    "    #------------------------------------------------------------\n",
    "    #choose an activation function\n",
    "\n",
    "    adam = Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    #-------------------------------------------------------------\n",
    "    #train a model\n",
    "\n",
    "    temp += 1\n",
    "    print('Started fitting model (leave out subject: ' + str(index) + ') ....... ' + str(temp))\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs = 75, validation_data=(X_test, y_test), verbose=0)\n",
    "    results.append(model.evaluate(X_test,y_test))\n",
    "    print('')\n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    print('Finished fitting model (leave out subject: ' + str(index) + ') ....... ' + str(temp))\n",
    "    print(\"Total time: \" + str(t1-t0))\n",
    "    print('------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss .............. 2.13333270887\n",
      "Average accuracy .......... 0.669239874441\n"
     ]
    }
   ],
   "source": [
    "avg = np.average(results, axis = 0)\n",
    "\n",
    "print('Average loss .............. ' + str(avg[0]))\n",
    "print('Average accuracy .......... ' + str(avg[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code hasnt been tested past this point !!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 1000, 1)           0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 1000, 32)          64        \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 1000, 32)          8320      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 13)                429       \n",
      "=================================================================\n",
      "Total params: 17,133\n",
      "Trainable params: 17,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "x = Dense(n_hidden, activation=\"relu\", name=\"fc1\")(input)\n",
    "x = LSTM(n_hidden, return_sequences=True, name='lstm_1')(x)\n",
    "x = LSTM(n_hidden, name='lstm_2')(x)\n",
    "x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "model = Model(input,x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs = 1, validation_data=(X_test, y_test), verbose=2) \n",
    "\n",
    "#only if using a sample, takes a lot of power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic bidirectional stacked LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 1000, 1)           0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 1000, 32)          64        \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 1000, 64)          16640     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 41,666\n",
      "Trainable params: 41,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "x = Dense(n_hidden, activation=\"relu\", name=\"fc1\")(input)\n",
    "x = Bidirectional(LSTM(n_hidden, return_sequences=True, name='lstm_1'))(x)\n",
    "x = Bidirectional(LSTM(n_hidden, name='lstm_2'))(x)\n",
    "x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "model = Model(input,x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/2\n",
      "24s - loss: 0.8445 - acc: 0.1900 - val_loss: 0.8392 - val_acc: 0.6000\n",
      "Epoch 2/2\n",
      "23s - loss: 0.8298 - acc: 0.8100 - val_loss: 0.8347 - val_acc: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d047ae0dd8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs = 2, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3x3 HAR stacked residual bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "lambda_loss_amount = 0.005\n",
    "clip_gradients = 15.0\n",
    "n_layers_in_highway = 0\n",
    "n_stacked_layers = 3\n",
    "n_hidden = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input (InputLayer)               (None, 1000, 1)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "fc1 (Dense)                      (None, 1000, 32)      64          input[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "bi_lstm1 (Bidirectional)         (None, 1000, 32)      6272        fc1[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "fc2 (Dense)                      (None, 1000, 32)      1056        bi_lstm1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bi_lstm2 (Bidirectional)         (None, 1000, 32)      6272        fc2[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "fc3 (Dense)                      (None, 1000, 32)      1056        bi_lstm2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_1 (Add)                      (None, 1000, 32)      0           fc2[0][0]                        \n",
      "                                                                   fc3[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "bi_lstm3 (Bidirectional)         (None, 1000, 32)      6272        add_1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "fc4 (Dense)                      (None, 1000, 32)      1056        bi_lstm3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bi_lstm4 (Bidirectional)         (None, 1000, 32)      6272        fc4[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "fc5 (Dense)                      (None, 1000, 32)      1056        bi_lstm4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_2 (Add)                      (None, 1000, 32)      0           fc4[0][0]                        \n",
      "                                                                   fc5[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "bi_lstm5 (Bidirectional)         (None, 1000, 32)      6272        add_2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "fc6 (Dense)                      (None, 1000, 32)      1056        bi_lstm5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bi_lstm6 (Bidirectional)         (None, 1000, 32)      6272        fc6[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "fc7 (Dense)                      (None, 1000, 32)      1056        bi_lstm6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "add_3 (Add)                      (None, 1000, 32)      0           fc6[0][0]                        \n",
      "                                                                   fc7[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)              (None, 32000)         0           add_3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "prediction (Dense)               (None, 2)             64002       flatten_4[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 108,034\n",
      "Trainable params: 108,034\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "x = Dense(32, activation=\"relu\", name=\"fc1\")(input)\n",
    "x = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm1')(x)\n",
    "x = Dense(32, activation=\"relu\", name=\"fc2\")(x)\n",
    "shortcut = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm2')(x)\n",
    "shortcut = Dense(32, activation=\"relu\", name=\"fc3\")(shortcut)\n",
    "x = layers.add([x, shortcut])\n",
    "# x = BatchNormalization(name='bn1')(x)\n",
    "\n",
    "x = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm3')(x)\n",
    "x = Dense(32, activation=\"relu\", name=\"fc4\")(x)\n",
    "shortcut = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm4')(x)\n",
    "shortcut = Dense(32, activation=\"relu\", name=\"fc5\")(shortcut)\n",
    "x = layers.add([x, shortcut])\n",
    "# x = BatchNormalization(name='bn2')(x)\n",
    "\n",
    "x = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm5')(x)\n",
    "x = Dense(32, activation=\"relu\", name=\"fc6\")(x)\n",
    "shortcut = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm6')(x)\n",
    "shortcut = Dense(32, activation=\"relu\", name=\"fc7\")(shortcut)\n",
    "x = layers.add([x, shortcut])\n",
    "# x = BatchNormalization(name='bn2')(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "\n",
    "model = Model(input,x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 200 samples\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs = 20, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 128, 9)            0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 128, 32)           320       \n",
      "_________________________________________________________________\n",
      "bi_lstm1 (Bidirectional)     (None, 128, 64)           16640     \n",
      "_________________________________________________________________\n",
      "bi_lstm2 (Bidirectional)     (None, 128, 64)           24832     \n",
      "_________________________________________________________________\n",
      "bi_lstm3 (Bidirectional)     (None, 128, 64)           24832     \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 128, 32)           2080      \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 6)                 24582     \n",
      "=================================================================\n",
      "Total params: 93,286\n",
      "Trainable params: 93,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "x = Dense(32, activation=\"relu\", name=\"fc1\")(input)\n",
    "x = Bidirectional(LSTM(32, return_sequences=True), name='bi_lstm1')(x)\n",
    "x = Bidirectional(LSTM(32, return_sequences=True), name='bi_lstm2')(x)\n",
    "x = Bidirectional(LSTM(32, return_sequences=True), name='bi_lstm3')(x)\n",
    "x = Dense(32, activation=\"relu\", name=\"fc2\")(x)\n",
    "# x = BatchNormalization(name='bn1')(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "\n",
    "model = Model(input,x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/200\n",
      "22s - loss: 2.0107 - acc: 0.3598 - val_loss: 1.8702 - val_acc: 0.4723\n",
      "Epoch 2/200\n",
      "3s - loss: 1.7907 - acc: 0.5652 - val_loss: 1.7388 - val_acc: 0.6162\n",
      "Epoch 3/200\n",
      "3s - loss: 1.6504 - acc: 0.7040 - val_loss: 1.6724 - val_acc: 0.7116\n",
      "Epoch 4/200\n",
      "3s - loss: 1.6000 - acc: 0.7695 - val_loss: 1.6468 - val_acc: 0.7333\n",
      "Epoch 5/200\n",
      "3s - loss: 1.5706 - acc: 0.8070 - val_loss: 1.6551 - val_acc: 0.7299\n",
      "Epoch 6/200\n",
      "3s - loss: 1.5382 - acc: 0.8368 - val_loss: 1.6104 - val_acc: 0.7764\n",
      "Epoch 7/200\n",
      "3s - loss: 1.5109 - acc: 0.8760 - val_loss: 1.5975 - val_acc: 0.7998\n",
      "Epoch 8/200\n",
      "3s - loss: 1.4862 - acc: 0.8915 - val_loss: 1.5667 - val_acc: 0.8419\n",
      "Epoch 9/200\n",
      "3s - loss: 1.4722 - acc: 0.9091 - val_loss: 1.5477 - val_acc: 0.8388\n",
      "Epoch 10/200\n",
      "3s - loss: 1.4598 - acc: 0.9221 - val_loss: 1.5361 - val_acc: 0.8497\n",
      "Epoch 11/200\n",
      "3s - loss: 1.4505 - acc: 0.9267 - val_loss: 1.5219 - val_acc: 0.8507\n",
      "Epoch 12/200\n",
      "3s - loss: 1.4439 - acc: 0.9334 - val_loss: 1.5163 - val_acc: 0.8588\n",
      "Epoch 13/200\n",
      "3s - loss: 1.4384 - acc: 0.9353 - val_loss: 1.5012 - val_acc: 0.8741\n",
      "Epoch 14/200\n",
      "3s - loss: 1.4349 - acc: 0.9385 - val_loss: 1.4979 - val_acc: 0.8761\n",
      "Epoch 15/200\n",
      "3s - loss: 1.4294 - acc: 0.9425 - val_loss: 1.4984 - val_acc: 0.8755\n",
      "Epoch 16/200\n",
      "3s - loss: 1.4261 - acc: 0.9431 - val_loss: 1.4919 - val_acc: 0.8843\n",
      "Epoch 17/200\n",
      "3s - loss: 1.4229 - acc: 0.9468 - val_loss: 1.4985 - val_acc: 0.8829\n",
      "Epoch 18/200\n",
      "3s - loss: 1.4219 - acc: 0.9493 - val_loss: 1.4871 - val_acc: 0.8880\n",
      "Epoch 19/200\n",
      "3s - loss: 1.4211 - acc: 0.9502 - val_loss: 1.4851 - val_acc: 0.8928\n",
      "Epoch 20/200\n",
      "3s - loss: 1.4182 - acc: 0.9495 - val_loss: 1.4842 - val_acc: 0.8968\n",
      "Epoch 21/200\n",
      "3s - loss: 1.4178 - acc: 0.9501 - val_loss: 1.4850 - val_acc: 0.8887\n",
      "Epoch 22/200\n",
      "3s - loss: 1.4182 - acc: 0.9489 - val_loss: 1.4821 - val_acc: 0.8979\n",
      "Epoch 23/200\n",
      "3s - loss: 1.4169 - acc: 0.9527 - val_loss: 1.4776 - val_acc: 0.9060\n",
      "Epoch 24/200\n",
      "3s - loss: 1.4133 - acc: 0.9525 - val_loss: 1.4804 - val_acc: 0.8989\n",
      "Epoch 25/200\n",
      "3s - loss: 1.4116 - acc: 0.9504 - val_loss: 1.4774 - val_acc: 0.9030\n",
      "Epoch 26/200\n",
      "3s - loss: 1.4101 - acc: 0.9508 - val_loss: 1.4773 - val_acc: 0.9091\n",
      "Epoch 27/200\n",
      "3s - loss: 1.4114 - acc: 0.9487 - val_loss: 1.4724 - val_acc: 0.8985\n",
      "Epoch 28/200\n",
      "3s - loss: 1.4101 - acc: 0.9521 - val_loss: 1.4753 - val_acc: 0.9111\n",
      "Epoch 29/200\n",
      "3s - loss: 1.4088 - acc: 0.9518 - val_loss: 1.4698 - val_acc: 0.9084\n",
      "Epoch 30/200\n",
      "3s - loss: 1.4069 - acc: 0.9538 - val_loss: 1.4705 - val_acc: 0.9094\n",
      "Epoch 31/200\n",
      "3s - loss: 1.4065 - acc: 0.9518 - val_loss: 1.4725 - val_acc: 0.9128\n",
      "Epoch 32/200\n",
      "3s - loss: 1.4073 - acc: 0.9513 - val_loss: 1.4685 - val_acc: 0.9077\n",
      "Epoch 33/200\n",
      "3s - loss: 1.4077 - acc: 0.9536 - val_loss: 1.4670 - val_acc: 0.9091\n",
      "Epoch 34/200\n",
      "3s - loss: 1.4080 - acc: 0.9539 - val_loss: 1.4906 - val_acc: 0.9013\n",
      "Epoch 35/200\n",
      "3s - loss: 1.4120 - acc: 0.9487 - val_loss: 1.4725 - val_acc: 0.9040\n",
      "Epoch 36/200\n",
      "3s - loss: 1.4076 - acc: 0.9510 - val_loss: 1.4726 - val_acc: 0.9043\n",
      "Epoch 37/200\n",
      "3s - loss: 1.4064 - acc: 0.9487 - val_loss: 1.4703 - val_acc: 0.9053\n",
      "Epoch 38/200\n",
      "3s - loss: 1.4041 - acc: 0.9550 - val_loss: 1.4674 - val_acc: 0.9084\n",
      "Epoch 39/200\n",
      "3s - loss: 1.4044 - acc: 0.9550 - val_loss: 1.4652 - val_acc: 0.9121\n",
      "Epoch 40/200\n",
      "3s - loss: 1.4032 - acc: 0.9557 - val_loss: 1.4664 - val_acc: 0.9114\n",
      "Epoch 41/200\n",
      "3s - loss: 1.4018 - acc: 0.9558 - val_loss: 1.4711 - val_acc: 0.9128\n",
      "Epoch 42/200\n",
      "3s - loss: 1.4011 - acc: 0.9557 - val_loss: 1.4695 - val_acc: 0.9138\n",
      "Epoch 43/200\n",
      "3s - loss: 1.4034 - acc: 0.9532 - val_loss: 1.4635 - val_acc: 0.9128\n",
      "Epoch 44/200\n",
      "3s - loss: 1.4006 - acc: 0.9542 - val_loss: 1.4723 - val_acc: 0.9118\n",
      "Epoch 45/200\n",
      "3s - loss: 1.4028 - acc: 0.9521 - val_loss: 1.4669 - val_acc: 0.9145\n",
      "Epoch 46/200\n",
      "3s - loss: 1.4025 - acc: 0.9524 - val_loss: 1.4662 - val_acc: 0.9108\n",
      "Epoch 47/200\n",
      "3s - loss: 1.4009 - acc: 0.9548 - val_loss: 1.4651 - val_acc: 0.9097\n",
      "Epoch 48/200\n",
      "3s - loss: 1.4000 - acc: 0.9553 - val_loss: 1.4656 - val_acc: 0.9097\n",
      "Epoch 49/200\n",
      "3s - loss: 1.4004 - acc: 0.9538 - val_loss: 1.4656 - val_acc: 0.9101\n",
      "Epoch 50/200\n",
      "3s - loss: 1.3985 - acc: 0.9562 - val_loss: 1.4601 - val_acc: 0.9118\n",
      "Epoch 51/200\n",
      "3s - loss: 1.3972 - acc: 0.9566 - val_loss: 1.4654 - val_acc: 0.9152\n",
      "Epoch 52/200\n",
      "3s - loss: 1.3981 - acc: 0.9587 - val_loss: 1.4619 - val_acc: 0.9142\n",
      "Epoch 53/200\n",
      "3s - loss: 1.3984 - acc: 0.9566 - val_loss: 1.4608 - val_acc: 0.9158\n",
      "Epoch 54/200\n",
      "3s - loss: 1.3984 - acc: 0.9577 - val_loss: 1.4618 - val_acc: 0.9080\n",
      "Epoch 55/200\n",
      "3s - loss: 1.3984 - acc: 0.9559 - val_loss: 1.4613 - val_acc: 0.9145\n",
      "Epoch 56/200\n",
      "3s - loss: 1.4004 - acc: 0.9572 - val_loss: 1.4627 - val_acc: 0.9125\n",
      "Epoch 57/200\n",
      "3s - loss: 1.3990 - acc: 0.9581 - val_loss: 1.4622 - val_acc: 0.9145\n",
      "Epoch 58/200\n",
      "3s - loss: 1.3972 - acc: 0.9585 - val_loss: 1.4622 - val_acc: 0.9131\n",
      "Epoch 59/200\n",
      "3s - loss: 1.3964 - acc: 0.9576 - val_loss: 1.4598 - val_acc: 0.9155\n",
      "Epoch 60/200\n",
      "3s - loss: 1.3980 - acc: 0.9577 - val_loss: 1.4586 - val_acc: 0.9199\n",
      "Epoch 61/200\n",
      "3s - loss: 1.4009 - acc: 0.9562 - val_loss: 1.4588 - val_acc: 0.9216\n",
      "Epoch 62/200\n",
      "3s - loss: 1.3991 - acc: 0.9565 - val_loss: 1.4645 - val_acc: 0.9182\n",
      "Epoch 63/200\n",
      "3s - loss: 1.3970 - acc: 0.9578 - val_loss: 1.4516 - val_acc: 0.9267\n",
      "Epoch 64/200\n",
      "3s - loss: 1.3951 - acc: 0.9555 - val_loss: 1.4555 - val_acc: 0.9179\n",
      "Epoch 65/200\n",
      "3s - loss: 1.3956 - acc: 0.9577 - val_loss: 1.4545 - val_acc: 0.9223\n",
      "Epoch 66/200\n",
      "3s - loss: 1.3955 - acc: 0.9574 - val_loss: 1.4556 - val_acc: 0.9169\n",
      "Epoch 67/200\n",
      "3s - loss: 1.3953 - acc: 0.9585 - val_loss: 1.4584 - val_acc: 0.9162\n",
      "Epoch 68/200\n",
      "3s - loss: 1.3941 - acc: 0.9585 - val_loss: 1.4577 - val_acc: 0.9131\n",
      "Epoch 69/200\n",
      "3s - loss: 1.3961 - acc: 0.9587 - val_loss: 1.4580 - val_acc: 0.9155\n",
      "Epoch 70/200\n",
      "3s - loss: 1.3945 - acc: 0.9572 - val_loss: 1.4500 - val_acc: 0.9253\n",
      "Epoch 71/200\n",
      "3s - loss: 1.3947 - acc: 0.9577 - val_loss: 1.4532 - val_acc: 0.9192\n",
      "Epoch 72/200\n",
      "3s - loss: 1.3938 - acc: 0.9593 - val_loss: 1.4524 - val_acc: 0.9243\n",
      "Epoch 73/200\n",
      "3s - loss: 1.3933 - acc: 0.9588 - val_loss: 1.4543 - val_acc: 0.9206\n",
      "Epoch 74/200\n",
      "3s - loss: 1.3930 - acc: 0.9608 - val_loss: 1.4557 - val_acc: 0.9192\n",
      "Epoch 75/200\n",
      "3s - loss: 1.3943 - acc: 0.9600 - val_loss: 1.4524 - val_acc: 0.9213\n",
      "Epoch 76/200\n",
      "3s - loss: 1.3919 - acc: 0.9603 - val_loss: 1.4520 - val_acc: 0.9220\n",
      "Epoch 77/200\n",
      "3s - loss: 1.3930 - acc: 0.9604 - val_loss: 1.4516 - val_acc: 0.9220\n",
      "Epoch 78/200\n",
      "3s - loss: 1.3930 - acc: 0.9606 - val_loss: 1.4487 - val_acc: 0.9230\n",
      "Epoch 79/200\n",
      "3s - loss: 1.3912 - acc: 0.9601 - val_loss: 1.4491 - val_acc: 0.9240\n",
      "Epoch 80/200\n",
      "3s - loss: 1.3903 - acc: 0.9616 - val_loss: 1.4488 - val_acc: 0.9155\n",
      "Epoch 81/200\n",
      "3s - loss: 1.3911 - acc: 0.9603 - val_loss: 1.4489 - val_acc: 0.9253\n",
      "Epoch 82/200\n",
      "3s - loss: 1.3920 - acc: 0.9621 - val_loss: 1.4482 - val_acc: 0.9203\n",
      "Epoch 83/200\n",
      "3s - loss: 1.3920 - acc: 0.9616 - val_loss: 1.4505 - val_acc: 0.9247\n",
      "Epoch 84/200\n",
      "3s - loss: 1.3915 - acc: 0.9627 - val_loss: 1.4501 - val_acc: 0.9203\n",
      "Epoch 85/200\n",
      "3s - loss: 1.3917 - acc: 0.9610 - val_loss: 1.4537 - val_acc: 0.9226\n",
      "Epoch 86/200\n",
      "3s - loss: 1.3914 - acc: 0.9612 - val_loss: 1.4478 - val_acc: 0.9253\n",
      "Epoch 87/200\n",
      "3s - loss: 1.3899 - acc: 0.9638 - val_loss: 1.4587 - val_acc: 0.9226\n",
      "Epoch 88/200\n",
      "3s - loss: 1.3914 - acc: 0.9610 - val_loss: 1.4491 - val_acc: 0.9260\n",
      "Epoch 89/200\n",
      "3s - loss: 1.3922 - acc: 0.9584 - val_loss: 1.4556 - val_acc: 0.9308\n",
      "Epoch 90/200\n",
      "3s - loss: 1.3917 - acc: 0.9604 - val_loss: 1.4521 - val_acc: 0.9233\n",
      "Epoch 91/200\n",
      "3s - loss: 1.3902 - acc: 0.9634 - val_loss: 1.4542 - val_acc: 0.9264\n",
      "Epoch 92/200\n",
      "3s - loss: 1.3882 - acc: 0.9646 - val_loss: 1.4523 - val_acc: 0.9267\n",
      "Epoch 93/200\n",
      "3s - loss: 1.3894 - acc: 0.9633 - val_loss: 1.4439 - val_acc: 0.9270\n",
      "Epoch 94/200\n",
      "3s - loss: 1.3878 - acc: 0.9668 - val_loss: 1.4538 - val_acc: 0.9250\n",
      "Epoch 95/200\n",
      "3s - loss: 1.3867 - acc: 0.9648 - val_loss: 1.4511 - val_acc: 0.9233\n",
      "Epoch 96/200\n",
      "3s - loss: 1.3870 - acc: 0.9646 - val_loss: 1.4455 - val_acc: 0.9294\n",
      "Epoch 97/200\n",
      "3s - loss: 1.3885 - acc: 0.9668 - val_loss: 1.4531 - val_acc: 0.9267\n",
      "Epoch 98/200\n",
      "3s - loss: 1.3884 - acc: 0.9678 - val_loss: 1.4497 - val_acc: 0.9274\n",
      "Epoch 99/200\n",
      "3s - loss: 1.3872 - acc: 0.9657 - val_loss: 1.4561 - val_acc: 0.9179\n",
      "Epoch 100/200\n",
      "3s - loss: 1.3892 - acc: 0.9641 - val_loss: 1.4505 - val_acc: 0.9284\n",
      "Epoch 101/200\n",
      "3s - loss: 1.3863 - acc: 0.9674 - val_loss: 1.4506 - val_acc: 0.9270\n",
      "Epoch 102/200\n",
      "3s - loss: 1.3874 - acc: 0.9646 - val_loss: 1.4491 - val_acc: 0.9209\n",
      "Epoch 103/200\n",
      "3s - loss: 1.3889 - acc: 0.9650 - val_loss: 1.4535 - val_acc: 0.9294\n",
      "Epoch 104/200\n",
      "3s - loss: 1.3864 - acc: 0.9664 - val_loss: 1.4572 - val_acc: 0.9237\n",
      "Epoch 105/200\n",
      "3s - loss: 1.3863 - acc: 0.9664 - val_loss: 1.4434 - val_acc: 0.9287\n",
      "Epoch 106/200\n",
      "3s - loss: 1.3839 - acc: 0.9683 - val_loss: 1.4527 - val_acc: 0.9274\n",
      "Epoch 107/200\n",
      "3s - loss: 1.3836 - acc: 0.9661 - val_loss: 1.4544 - val_acc: 0.9233\n",
      "Epoch 108/200\n",
      "3s - loss: 1.3832 - acc: 0.9680 - val_loss: 1.4532 - val_acc: 0.9203\n",
      "Epoch 109/200\n",
      "3s - loss: 1.3831 - acc: 0.9709 - val_loss: 1.4498 - val_acc: 0.9281\n",
      "Epoch 110/200\n",
      "3s - loss: 1.3818 - acc: 0.9709 - val_loss: 1.4582 - val_acc: 0.9270\n",
      "Epoch 111/200\n",
      "3s - loss: 1.3821 - acc: 0.9699 - val_loss: 1.4574 - val_acc: 0.9270\n",
      "Epoch 112/200\n",
      "3s - loss: 1.3855 - acc: 0.9675 - val_loss: 1.4549 - val_acc: 0.9277\n",
      "Epoch 113/200\n",
      "3s - loss: 1.3868 - acc: 0.9678 - val_loss: 1.4481 - val_acc: 0.9342\n",
      "Epoch 114/200\n",
      "3s - loss: 1.3853 - acc: 0.9710 - val_loss: 1.4552 - val_acc: 0.9243\n",
      "Epoch 115/200\n",
      "3s - loss: 1.3842 - acc: 0.9682 - val_loss: 1.4460 - val_acc: 0.9291\n",
      "Epoch 116/200\n",
      "3s - loss: 1.3830 - acc: 0.9716 - val_loss: 1.4567 - val_acc: 0.9247\n",
      "Epoch 117/200\n",
      "3s - loss: 1.3819 - acc: 0.9709 - val_loss: 1.4518 - val_acc: 0.9250\n",
      "Epoch 118/200\n",
      "3s - loss: 1.3847 - acc: 0.9695 - val_loss: 1.4660 - val_acc: 0.9250\n",
      "Epoch 119/200\n",
      "3s - loss: 1.3884 - acc: 0.9683 - val_loss: 1.4501 - val_acc: 0.9257\n",
      "Epoch 120/200\n",
      "3s - loss: 1.3856 - acc: 0.9686 - val_loss: 1.4448 - val_acc: 0.9321\n",
      "Epoch 121/200\n",
      "3s - loss: 1.3833 - acc: 0.9667 - val_loss: 1.4495 - val_acc: 0.9250\n",
      "Epoch 122/200\n",
      "3s - loss: 1.3811 - acc: 0.9718 - val_loss: 1.4581 - val_acc: 0.9301\n",
      "Epoch 123/200\n",
      "3s - loss: 1.3820 - acc: 0.9710 - val_loss: 1.4621 - val_acc: 0.9230\n",
      "Epoch 124/200\n",
      "3s - loss: 1.3860 - acc: 0.9708 - val_loss: 1.4625 - val_acc: 0.9209\n",
      "Epoch 125/200\n",
      "3s - loss: 1.3883 - acc: 0.9659 - val_loss: 1.4547 - val_acc: 0.9220\n",
      "Epoch 126/200\n",
      "3s - loss: 1.3846 - acc: 0.9693 - val_loss: 1.4503 - val_acc: 0.9318\n",
      "Epoch 127/200\n",
      "3s - loss: 1.3829 - acc: 0.9693 - val_loss: 1.4502 - val_acc: 0.9294\n",
      "Epoch 128/200\n",
      "3s - loss: 1.3826 - acc: 0.9686 - val_loss: 1.4526 - val_acc: 0.9247\n",
      "Epoch 129/200\n",
      "3s - loss: 1.3837 - acc: 0.9697 - val_loss: 1.4535 - val_acc: 0.9304\n",
      "Epoch 130/200\n",
      "3s - loss: 1.3826 - acc: 0.9709 - val_loss: 1.4534 - val_acc: 0.9203\n",
      "Epoch 131/200\n",
      "3s - loss: 1.3829 - acc: 0.9697 - val_loss: 1.4512 - val_acc: 0.9260\n",
      "Epoch 132/200\n",
      "3s - loss: 1.3812 - acc: 0.9717 - val_loss: 1.4548 - val_acc: 0.9182\n",
      "Epoch 133/200\n",
      "3s - loss: 1.3807 - acc: 0.9720 - val_loss: 1.4539 - val_acc: 0.9230\n",
      "Epoch 134/200\n",
      "3s - loss: 1.3798 - acc: 0.9729 - val_loss: 1.4574 - val_acc: 0.9135\n",
      "Epoch 135/200\n",
      "3s - loss: 1.3803 - acc: 0.9727 - val_loss: 1.4618 - val_acc: 0.9250\n",
      "Epoch 136/200\n",
      "3s - loss: 1.3795 - acc: 0.9731 - val_loss: 1.4616 - val_acc: 0.9213\n",
      "Epoch 137/200\n",
      "3s - loss: 1.3786 - acc: 0.9727 - val_loss: 1.4617 - val_acc: 0.9223\n",
      "Epoch 138/200\n",
      "3s - loss: 1.3783 - acc: 0.9743 - val_loss: 1.4627 - val_acc: 0.9203\n",
      "Epoch 139/200\n",
      "3s - loss: 1.3787 - acc: 0.9737 - val_loss: 1.4551 - val_acc: 0.9281\n",
      "Epoch 140/200\n",
      "3s - loss: 1.3793 - acc: 0.9737 - val_loss: 1.4553 - val_acc: 0.9182\n",
      "Epoch 141/200\n",
      "3s - loss: 1.3800 - acc: 0.9766 - val_loss: 1.4547 - val_acc: 0.9267\n",
      "Epoch 142/200\n",
      "3s - loss: 1.3785 - acc: 0.9737 - val_loss: 1.4555 - val_acc: 0.9233\n",
      "Epoch 143/200\n",
      "3s - loss: 1.3777 - acc: 0.9732 - val_loss: 1.4571 - val_acc: 0.9240\n",
      "Epoch 144/200\n",
      "3s - loss: 1.3787 - acc: 0.9748 - val_loss: 1.4587 - val_acc: 0.9220\n",
      "Epoch 145/200\n",
      "3s - loss: 1.3775 - acc: 0.9746 - val_loss: 1.4645 - val_acc: 0.9274\n",
      "Epoch 146/200\n",
      "3s - loss: 1.3781 - acc: 0.9763 - val_loss: 1.4607 - val_acc: 0.9284\n",
      "Epoch 147/200\n",
      "3s - loss: 1.3805 - acc: 0.9727 - val_loss: 1.4498 - val_acc: 0.9321\n",
      "Epoch 148/200\n",
      "3s - loss: 1.3809 - acc: 0.9740 - val_loss: 1.4609 - val_acc: 0.9240\n",
      "Epoch 149/200\n",
      "3s - loss: 1.3800 - acc: 0.9728 - val_loss: 1.4617 - val_acc: 0.9247\n",
      "Epoch 150/200\n",
      "3s - loss: 1.3795 - acc: 0.9742 - val_loss: 1.4597 - val_acc: 0.9315\n",
      "Epoch 151/200\n",
      "3s - loss: 1.3796 - acc: 0.9729 - val_loss: 1.4590 - val_acc: 0.9311\n",
      "Epoch 152/200\n",
      "3s - loss: 1.3802 - acc: 0.9724 - val_loss: 1.4579 - val_acc: 0.9304\n",
      "Epoch 153/200\n",
      "3s - loss: 1.3790 - acc: 0.9732 - val_loss: 1.4506 - val_acc: 0.9281\n",
      "Epoch 154/200\n",
      "3s - loss: 1.3786 - acc: 0.9748 - val_loss: 1.4537 - val_acc: 0.9325\n",
      "Epoch 155/200\n",
      "3s - loss: 1.3770 - acc: 0.9739 - val_loss: 1.4593 - val_acc: 0.9223\n",
      "Epoch 156/200\n",
      "3s - loss: 1.3780 - acc: 0.9761 - val_loss: 1.4734 - val_acc: 0.9216\n",
      "Epoch 157/200\n",
      "3s - loss: 1.3793 - acc: 0.9747 - val_loss: 1.4564 - val_acc: 0.9267\n",
      "Epoch 158/200\n",
      "3s - loss: 1.3777 - acc: 0.9751 - val_loss: 1.4568 - val_acc: 0.9304\n",
      "Epoch 159/200\n",
      "3s - loss: 1.3752 - acc: 0.9763 - val_loss: 1.4641 - val_acc: 0.9247\n",
      "Epoch 160/200\n",
      "3s - loss: 1.3762 - acc: 0.9763 - val_loss: 1.4656 - val_acc: 0.9250\n",
      "Epoch 161/200\n",
      "3s - loss: 1.3776 - acc: 0.9751 - val_loss: 1.4626 - val_acc: 0.9253\n",
      "Epoch 162/200\n",
      "3s - loss: 1.3776 - acc: 0.9785 - val_loss: 1.4610 - val_acc: 0.9311\n",
      "Epoch 163/200\n",
      "3s - loss: 1.3762 - acc: 0.9770 - val_loss: 1.4609 - val_acc: 0.9247\n",
      "Epoch 164/200\n",
      "3s - loss: 1.3757 - acc: 0.9786 - val_loss: 1.4652 - val_acc: 0.9264\n",
      "Epoch 165/200\n",
      "3s - loss: 1.3760 - acc: 0.9763 - val_loss: 1.4605 - val_acc: 0.9287\n",
      "Epoch 166/200\n",
      "3s - loss: 1.3762 - acc: 0.9762 - val_loss: 1.4502 - val_acc: 0.9301\n",
      "Epoch 167/200\n",
      "3s - loss: 1.3748 - acc: 0.9793 - val_loss: 1.4618 - val_acc: 0.9342\n",
      "Epoch 168/200\n",
      "3s - loss: 1.3746 - acc: 0.9800 - val_loss: 1.4626 - val_acc: 0.9304\n",
      "Epoch 169/200\n",
      "3s - loss: 1.3743 - acc: 0.9800 - val_loss: 1.4661 - val_acc: 0.9277\n",
      "Epoch 170/200\n",
      "3s - loss: 1.3745 - acc: 0.9792 - val_loss: 1.4569 - val_acc: 0.9348\n",
      "Epoch 171/200\n",
      "3s - loss: 1.3753 - acc: 0.9791 - val_loss: 1.4535 - val_acc: 0.9274\n",
      "Epoch 172/200\n",
      "3s - loss: 1.3742 - acc: 0.9825 - val_loss: 1.4512 - val_acc: 0.9382\n",
      "Epoch 173/200\n",
      "3s - loss: 1.3741 - acc: 0.9820 - val_loss: 1.4500 - val_acc: 0.9372\n",
      "Epoch 174/200\n",
      "3s - loss: 1.3737 - acc: 0.9831 - val_loss: 1.4534 - val_acc: 0.9376\n",
      "Epoch 175/200\n",
      "3s - loss: 1.3724 - acc: 0.9833 - val_loss: 1.4529 - val_acc: 0.9318\n",
      "Epoch 176/200\n",
      "3s - loss: 1.3724 - acc: 0.9838 - val_loss: 1.4566 - val_acc: 0.9362\n",
      "Epoch 177/200\n",
      "3s - loss: 1.3723 - acc: 0.9844 - val_loss: 1.4547 - val_acc: 0.9328\n",
      "Epoch 178/200\n",
      "3s - loss: 1.3735 - acc: 0.9819 - val_loss: 1.4537 - val_acc: 0.9355\n",
      "Epoch 179/200\n",
      "3s - loss: 1.3728 - acc: 0.9838 - val_loss: 1.4616 - val_acc: 0.9287\n",
      "Epoch 180/200\n",
      "3s - loss: 1.3741 - acc: 0.9831 - val_loss: 1.4629 - val_acc: 0.9338\n",
      "Epoch 181/200\n",
      "3s - loss: 1.3731 - acc: 0.9808 - val_loss: 1.4484 - val_acc: 0.9386\n",
      "Epoch 182/200\n",
      "3s - loss: 1.3722 - acc: 0.9861 - val_loss: 1.4519 - val_acc: 0.9386\n",
      "Epoch 183/200\n",
      "3s - loss: 1.3720 - acc: 0.9849 - val_loss: 1.4507 - val_acc: 0.9427\n",
      "Epoch 184/200\n",
      "3s - loss: 1.3712 - acc: 0.9852 - val_loss: 1.4468 - val_acc: 0.9416\n",
      "Epoch 185/200\n",
      "3s - loss: 1.3705 - acc: 0.9861 - val_loss: 1.4566 - val_acc: 0.9420\n",
      "Epoch 186/200\n",
      "3s - loss: 1.3722 - acc: 0.9842 - val_loss: 1.4442 - val_acc: 0.9362\n",
      "Epoch 187/200\n",
      "3s - loss: 1.3749 - acc: 0.9826 - val_loss: 1.4552 - val_acc: 0.9376\n",
      "Epoch 188/200\n",
      "3s - loss: 1.3763 - acc: 0.9811 - val_loss: 1.4440 - val_acc: 0.9318\n",
      "Epoch 189/200\n",
      "3s - loss: 1.3743 - acc: 0.9812 - val_loss: 1.4449 - val_acc: 0.9372\n",
      "Epoch 190/200\n",
      "3s - loss: 1.3713 - acc: 0.9857 - val_loss: 1.4412 - val_acc: 0.9427\n",
      "Epoch 191/200\n",
      "3s - loss: 1.3709 - acc: 0.9850 - val_loss: 1.4428 - val_acc: 0.9403\n",
      "Epoch 192/200\n",
      "3s - loss: 1.3705 - acc: 0.9867 - val_loss: 1.4463 - val_acc: 0.9382\n",
      "Epoch 193/200\n",
      "3s - loss: 1.3698 - acc: 0.9871 - val_loss: 1.4560 - val_acc: 0.9328\n",
      "Epoch 194/200\n",
      "3s - loss: 1.3693 - acc: 0.9880 - val_loss: 1.4572 - val_acc: 0.9379\n",
      "Epoch 195/200\n",
      "3s - loss: 1.3710 - acc: 0.9863 - val_loss: 1.4615 - val_acc: 0.9308\n",
      "Epoch 196/200\n",
      "3s - loss: 1.3692 - acc: 0.9883 - val_loss: 1.4563 - val_acc: 0.9308\n",
      "Epoch 197/200\n",
      "3s - loss: 1.3702 - acc: 0.9859 - val_loss: 1.4537 - val_acc: 0.9338\n",
      "Epoch 198/200\n",
      "3s - loss: 1.3692 - acc: 0.9875 - val_loss: 1.4544 - val_acc: 0.9352\n",
      "Epoch 199/200\n",
      "3s - loss: 1.3689 - acc: 0.9899 - val_loss: 1.4548 - val_acc: 0.9369\n",
      "Epoch 200/200\n",
      "3s - loss: 1.3687 - acc: 0.9902 - val_loss: 1.4489 - val_acc: 0.9396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe55eaec748>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs = 200, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 128, 6)            0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 128, 32)           224       \n",
      "_________________________________________________________________\n",
      "bi_lstm1 (Bidirectional)     (None, 128, 64)           16640     \n",
      "_________________________________________________________________\n",
      "fc4 (Dense)                  (None, 128, 32)           2080      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 6)                 24582     \n",
      "=================================================================\n",
      "Total params: 43,526\n",
      "Trainable params: 43,526\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "x = Dense(32, activation=\"relu\", name=\"fc1\")(input)\n",
    "x = Bidirectional(LSTM(32, return_sequences=True), name='bi_lstm1')(x)\n",
    "# x = Bidirectional(LSTM(16, return_sequences=True), name='bi_lstm2')(x)\n",
    "x = Dense(32, activation=\"relu\", name=\"fc4\")(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "\n",
    "model = Model(input,x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected prediction to have shape (None, 6) but got array with shape (2754852, 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-5ca48c2a61c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1236\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1239\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1240\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    138\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected prediction to have shape (None, 6) but got array with shape (2754852, 18)"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs = 200, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 1000, 1)           0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 1000, 128)         256       \n",
      "_________________________________________________________________\n",
      "bi_lstm1 (Bidirectional)     (None, 1000, 256)         263168    \n",
      "_________________________________________________________________\n",
      "bi_lstm2 (Bidirectional)     (None, 1000, 256)         394240    \n",
      "_________________________________________________________________\n",
      "fc4 (Dense)                  (None, 1000, 128)         32896     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128000)            0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 2)                 256002    \n",
      "=================================================================\n",
      "Total params: 946,562\n",
      "Trainable params: 946,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "x = Dense(128, activation=\"relu\", name=\"fc1\")(input)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True), name='bi_lstm1')(x)\n",
    "x = Bidirectional(LSTM(128, return_sequences=True), name='bi_lstm2')(x)\n",
    "x = Dense(128, activation=\"relu\", name=\"fc4\")(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "\n",
    "model = Model(input,x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ConvLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 1125, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 1121, 64)          384       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 560, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv1D)               (None, 556, 64)           20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 278, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 17792)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 17792)             0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 19)                338067    \n",
      "=================================================================\n",
      "Total params: 358,995\n",
      "Trainable params: 358,995\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(n_steps, n_input), name=\"input\")\n",
    "x = Conv1D(64, 5, activation='relu', name='conv1')(input)\n",
    "x = MaxPool1D()(x)\n",
    "x = Conv1D(64, 5, activation='relu', name='conv2')(x)\n",
    "x = MaxPool1D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(.5)(x)\n",
    "x = Dense(n_classes, activation=\"softmax\", activity_regularizer=l2(lambda_loss_amount), name=\"prediction\")(x)\n",
    "\n",
    "model = Model(input,x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adam = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87780 samples, validate on 12540 samples\n",
      "Epoch 1/2\n",
      "309s - loss: 1.9597 - acc: 0.6588 - val_loss: 2.1602 - val_acc: 0.7309\n",
      "Epoch 2/2\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs = 2, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Conclusion: \n",
    "\n",
    "This computer cannot handle the used preprocessed dataset unlesss the said dataset is sampled to less than 1% of the original size\n",
    "Even then bidirectional LSTM doesnt go through even for 1 epoch, sigh"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
